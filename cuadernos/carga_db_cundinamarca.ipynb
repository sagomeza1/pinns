{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f67788",
   "metadata": {},
   "source": [
    "Cuaderno de python para almacenar los datos extraidos de datos abiertos en una base en sql server para su posterior procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "666e9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pyodbc\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb4413",
   "metadata": {},
   "source": [
    "# Cundinamarca Dic 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaae7b1",
   "metadata": {},
   "source": [
    "## Presión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f400b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de descarga e ingesta...\n",
      "Descargando registros desde 0...\n",
      "Procesadas 1000 filas...\n",
      "Descargando registros desde 1000...\n",
      "Procesadas 2000 filas...\n",
      "Descargando registros desde 2000...\n",
      "Procesadas 3000 filas...\n",
      "Descargando registros desde 3000...\n",
      "Procesadas 4000 filas...\n",
      "Descargando registros desde 4000...\n",
      "Procesadas 5000 filas...\n",
      "Descargando registros desde 5000...\n",
      "Procesadas 6000 filas...\n",
      "Descargando registros desde 6000...\n",
      "Procesadas 7000 filas...\n",
      "Descargando registros desde 7000...\n",
      "Procesadas 8000 filas...\n",
      "Descargando registros desde 8000...\n",
      "Procesadas 9000 filas...\n",
      "Descargando registros desde 9000...\n",
      "Procesadas 10000 filas...\n",
      "Descargando registros desde 10000...\n",
      "Procesadas 11000 filas...\n",
      "Descargando registros desde 11000...\n",
      "Procesadas 12000 filas...\n",
      "Descargando registros desde 12000...\n",
      "Procesadas 13000 filas...\n",
      "Descargando registros desde 13000...\n",
      "Procesadas 14000 filas...\n",
      "Descargando registros desde 14000...\n",
      "Procesadas 15000 filas...\n",
      "Descargando registros desde 15000...\n",
      "Procesadas 16000 filas...\n",
      "Descargando registros desde 16000...\n",
      "Procesadas 17000 filas...\n",
      "Descargando registros desde 17000...\n",
      "Procesadas 18000 filas...\n",
      "Descargando registros desde 18000...\n",
      "Procesadas 19000 filas...\n",
      "Descargando registros desde 19000...\n",
      "Procesadas 20000 filas...\n",
      "Descargando registros desde 20000...\n",
      "Procesadas 21000 filas...\n",
      "Descargando registros desde 21000...\n",
      "Procesadas 22000 filas...\n",
      "Descargando registros desde 22000...\n",
      "Procesadas 23000 filas...\n",
      "Descargando registros desde 23000...\n",
      "Procesadas 24000 filas...\n",
      "Descargando registros desde 24000...\n",
      "Procesadas 25000 filas...\n",
      "Descargando registros desde 25000...\n",
      "Procesadas 26000 filas...\n",
      "Descargando registros desde 26000...\n",
      "Procesadas 27000 filas...\n",
      "Descargando registros desde 27000...\n",
      "Procesadas 28000 filas...\n",
      "Descargando registros desde 28000...\n",
      "Procesadas 29000 filas...\n",
      "Descargando registros desde 29000...\n",
      "Procesadas 30000 filas...\n",
      "Descargando registros desde 30000...\n",
      "Procesadas 31000 filas...\n",
      "Descargando registros desde 31000...\n",
      "Procesadas 32000 filas...\n",
      "Descargando registros desde 32000...\n",
      "Procesadas 33000 filas...\n",
      "Descargando registros desde 33000...\n",
      "Procesadas 34000 filas...\n",
      "Descargando registros desde 34000...\n",
      "Procesadas 35000 filas...\n",
      "Descargando registros desde 35000...\n",
      "Procesadas 36000 filas...\n",
      "Descargando registros desde 36000...\n",
      "Procesadas 37000 filas...\n",
      "Descargando registros desde 37000...\n",
      "Procesadas 38000 filas...\n",
      "Descargando registros desde 38000...\n",
      "Procesadas 39000 filas...\n",
      "Descargando registros desde 39000...\n",
      "Procesadas 40000 filas...\n",
      "Descargando registros desde 40000...\n",
      "Procesadas 41000 filas...\n",
      "Descargando registros desde 41000...\n",
      "Procesadas 42000 filas...\n",
      "Descargando registros desde 42000...\n",
      "Procesadas 43000 filas...\n",
      "Descargando registros desde 43000...\n",
      "Procesadas 44000 filas...\n",
      "Descargando registros desde 44000...\n",
      "Procesadas 45000 filas...\n",
      "Descargando registros desde 45000...\n",
      "Procesadas 46000 filas...\n",
      "Descargando registros desde 46000...\n",
      "Procesadas 47000 filas...\n",
      "Descargando registros desde 47000...\n",
      "Procesadas 48000 filas...\n",
      "Descargando registros desde 48000...\n",
      "Procesadas 49000 filas...\n",
      "Descargando registros desde 49000...\n",
      "Procesadas 50000 filas...\n",
      "Descargando registros desde 50000...\n",
      "Procesadas 51000 filas...\n",
      "Descargando registros desde 51000...\n",
      "Procesadas 52000 filas...\n",
      "Descargando registros desde 52000...\n",
      "Procesadas 53000 filas...\n",
      "Descargando registros desde 53000...\n",
      "Procesadas 54000 filas...\n",
      "Descargando registros desde 54000...\n",
      "Procesadas 54835 filas...\n",
      "Descargando registros desde 55000...\n",
      "Proceso finalizado: No hay más datos para descargar.\n",
      "Conexión cerrada.\n"
     ]
    }
   ],
   "source": [
    "# URL base de la API\n",
    "# URL sin paginación - la paginación se hará construyendo dinámicamente el query\n",
    "base_url_api = \"https://www.datos.gov.co/resource/62tk-nxj5.csv\"\n",
    "\n",
    "# Parámetros\n",
    "limit = 1000  # Reducido a 500 para queries más rápidas\n",
    "offset = 0    # desplazamiento inicial\n",
    "all_data = [] # lista para almacenar los bloques\n",
    "\n",
    "# Configuración de los parámetros de conexión\n",
    "server = 'localhost\\\\SQLEXPRESS'  # O el punto '.' que usaste en SSMS\n",
    "database = 'EM_CUN'\n",
    "\n",
    "# Cadena de conexión usando Autenticación de Windows (Trusted_Connection)\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO dbo.presion (\n",
    "        codigo_estacion, codigo_sensor, fecha_observacion, valor_observado,\n",
    "        nombre_estacion, departamento, municipio, zona_hidrografica,\n",
    "        latitud, longitud, descripcion_sensor, unidad_medida\n",
    "    ) \n",
    "    VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "\n",
    "# Función para reintentos\n",
    "import time\n",
    "def request_with_retries(url, max_retries=3, timeout=60):\n",
    "    \"\"\"Intenta hacer la solicitud hasta 3 veces con timeout de 60 segundos\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout)\n",
    "            return response\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout en intento {attempt + 1}/{max_retries}. Esperando 5 segundos...\")\n",
    "            time.sleep(5)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"Error de conexión en intento {attempt + 1}/{max_retries}: {e}\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    raise Exception(f\"No se pudo conectar después de {max_retries} intentos\")\n",
    "\n",
    "try:\n",
    "    # 1. Establecer conexión inicial\n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.fast_executemany = True\n",
    "        \n",
    "        print(\"Iniciando proceso de descarga e ingesta...\")\n",
    "\n",
    "        while True:\n",
    "            # 2. Construir el query SoQL con LIMIT y OFFSET incluidos\n",
    "            soql_query = f\"\"\"SELECT\n",
    "  `codigoestacion`,\n",
    "  `codigosensor`,\n",
    "  `fechaobservacion`,\n",
    "  `valorobservado`,\n",
    "  `nombreestacion`,\n",
    "  `departamento`,\n",
    "  `municipio`,\n",
    "  `zonahidrografica`,\n",
    "  `latitud`,\n",
    "  `longitud`,\n",
    "  `descripcionsensor`,\n",
    "  `unidadmedida`\n",
    "WHERE\n",
    "  `fechaobservacion`\n",
    "    BETWEEN \"2025-12-01T00:00:00\" :: floating_timestamp\n",
    "    AND \"2025-12-31T23:59:59\" :: floating_timestamp\n",
    "  AND caseless_eq(`departamento`, \"CUNDINAMARCA\")\n",
    "LIMIT {limit}\n",
    "OFFSET {offset}\"\"\"\n",
    "            \n",
    "            # URL-encode el query\n",
    "            import urllib.parse\n",
    "            encoded_query = urllib.parse.quote(soql_query)\n",
    "            url = f\"{base_url_api}?$query={encoded_query}\"\n",
    "            \n",
    "            response = request_with_retries(url, max_retries=3, timeout=60)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error en la API: {response.status_code}\")\n",
    "                print(f\"Respuesta: {response.text}\")\n",
    "                break\n",
    "\n",
    "            # 3. Leer bloque como DataFrame\n",
    "            df_chunk = pd.read_csv(StringIO(response.text), dtype=str)\n",
    "            \n",
    "            if df_chunk.empty:\n",
    "                print(\"Proceso finalizado: No hay más datos para descargar.\")\n",
    "                break\n",
    "            \n",
    "            # Convertir tipos de datos\n",
    "            df_chunk[\"valorobservado\"] = df_chunk[\"valorobservado\"].astype(float)\n",
    "            df_chunk[\"latitud\"] = df_chunk[\"latitud\"].astype(float)\n",
    "            df_chunk[\"longitud\"] = df_chunk[\"longitud\"].astype(float)  \n",
    "            df_chunk[\"fechaobservacion\"] = pd.to_datetime(df_chunk[\"fechaobservacion\"])            \n",
    "            \n",
    "            # 4. Cargar bloque actual a la base de datos\n",
    "            try:\n",
    "                records = df_chunk.values.tolist()\n",
    "                cursor.executemany(insert_query, records)\n",
    "                conn.commit()  # Commit por bloque para asegurar persistencia\n",
    "                \n",
    "                print(f\"Procesadas {offset + len(df_chunk)} filas...\")\n",
    "                \n",
    "            except Exception as e_db:\n",
    "                print(f\"Error insertando bloque en offset {offset}: {e_db}\")\n",
    "                conn.rollback()\n",
    "                break\n",
    "\n",
    "            # 5. Incrementar offset para el siguiente bloque\n",
    "            offset += limit\n",
    "\n",
    "except pyodbc.Error as e_conn:\n",
    "    print(f\"Error de conexión a la base de datos: {e_conn}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Conexión cerrada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef1d14",
   "metadata": {},
   "source": [
    "## Dirección del viento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f93d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de descarga e ingesta...\n",
      "Procesadas 1000 filas...\n",
      "Procesadas 2000 filas...\n",
      "Procesadas 3000 filas...\n",
      "Procesadas 4000 filas...\n",
      "Procesadas 5000 filas...\n",
      "Procesadas 6000 filas...\n",
      "Procesadas 7000 filas...\n",
      "Procesadas 8000 filas...\n",
      "Procesadas 9000 filas...\n",
      "Procesadas 10000 filas...\n",
      "Procesadas 11000 filas...\n",
      "Procesadas 12000 filas...\n",
      "Procesadas 13000 filas...\n",
      "Procesadas 14000 filas...\n",
      "Procesadas 15000 filas...\n",
      "Procesadas 16000 filas...\n",
      "Procesadas 17000 filas...\n",
      "Procesadas 18000 filas...\n",
      "Procesadas 19000 filas...\n",
      "Procesadas 20000 filas...\n",
      "Procesadas 21000 filas...\n",
      "Procesadas 22000 filas...\n",
      "Procesadas 23000 filas...\n",
      "Procesadas 24000 filas...\n",
      "Procesadas 25000 filas...\n",
      "Procesadas 26000 filas...\n",
      "Procesadas 27000 filas...\n",
      "Procesadas 28000 filas...\n",
      "Procesadas 29000 filas...\n",
      "Procesadas 30000 filas...\n",
      "Procesadas 31000 filas...\n",
      "Procesadas 32000 filas...\n",
      "Procesadas 33000 filas...\n",
      "Procesadas 34000 filas...\n",
      "Procesadas 35000 filas...\n",
      "Procesadas 36000 filas...\n",
      "Procesadas 37000 filas...\n",
      "Procesadas 38000 filas...\n",
      "Procesadas 39000 filas...\n",
      "Procesadas 40000 filas...\n",
      "Procesadas 41000 filas...\n",
      "Procesadas 42000 filas...\n",
      "Procesadas 43000 filas...\n",
      "Procesadas 44000 filas...\n",
      "Procesadas 45000 filas...\n",
      "Procesadas 46000 filas...\n",
      "Procesadas 47000 filas...\n",
      "Procesadas 48000 filas...\n",
      "Procesadas 49000 filas...\n",
      "Procesadas 50000 filas...\n",
      "Procesadas 51000 filas...\n",
      "Procesadas 52000 filas...\n",
      "Procesadas 53000 filas...\n",
      "Procesadas 54000 filas...\n",
      "Procesadas 55000 filas...\n",
      "Procesadas 56000 filas...\n",
      "Procesadas 57000 filas...\n",
      "Procesadas 58000 filas...\n",
      "Procesadas 59000 filas...\n",
      "Procesadas 60000 filas...\n",
      "Procesadas 61000 filas...\n",
      "Procesadas 62000 filas...\n",
      "Procesadas 63000 filas...\n",
      "Procesadas 64000 filas...\n",
      "Procesadas 65000 filas...\n",
      "Procesadas 66000 filas...\n",
      "Procesadas 67000 filas...\n",
      "Procesadas 68000 filas...\n",
      "Procesadas 69000 filas...\n",
      "Procesadas 70000 filas...\n",
      "Procesadas 71000 filas...\n",
      "Procesadas 72000 filas...\n",
      "Procesadas 73000 filas...\n",
      "Procesadas 74000 filas...\n",
      "Procesadas 75000 filas...\n",
      "Procesadas 76000 filas...\n",
      "Procesadas 77000 filas...\n",
      "Procesadas 78000 filas...\n",
      "Procesadas 79000 filas...\n",
      "Procesadas 80000 filas...\n",
      "Procesadas 81000 filas...\n",
      "Procesadas 82000 filas...\n",
      "Procesadas 83000 filas...\n",
      "Procesadas 84000 filas...\n",
      "Procesadas 85000 filas...\n",
      "Procesadas 86000 filas...\n",
      "Procesadas 87000 filas...\n",
      "Procesadas 88000 filas...\n",
      "Procesadas 89000 filas...\n",
      "Procesadas 90000 filas...\n",
      "Procesadas 91000 filas...\n",
      "Procesadas 92000 filas...\n",
      "Procesadas 93000 filas...\n",
      "Procesadas 94000 filas...\n",
      "Procesadas 95000 filas...\n",
      "Procesadas 96000 filas...\n",
      "Procesadas 97000 filas...\n",
      "Procesadas 98000 filas...\n",
      "Procesadas 99000 filas...\n",
      "Procesadas 100000 filas...\n",
      "Procesadas 101000 filas...\n",
      "Procesadas 102000 filas...\n",
      "Procesadas 103000 filas...\n",
      "Procesadas 104000 filas...\n",
      "Procesadas 105000 filas...\n",
      "Procesadas 106000 filas...\n",
      "Procesadas 107000 filas...\n",
      "Procesadas 108000 filas...\n",
      "Procesadas 109000 filas...\n",
      "Procesadas 110000 filas...\n",
      "Procesadas 111000 filas...\n",
      "Procesadas 112000 filas...\n",
      "Procesadas 113000 filas...\n",
      "Procesadas 114000 filas...\n",
      "Procesadas 115000 filas...\n",
      "Procesadas 116000 filas...\n",
      "Procesadas 117000 filas...\n",
      "Procesadas 118000 filas...\n",
      "Procesadas 119000 filas...\n",
      "Procesadas 120000 filas...\n",
      "Procesadas 121000 filas...\n",
      "Procesadas 122000 filas...\n",
      "Procesadas 123000 filas...\n",
      "Procesadas 124000 filas...\n",
      "Procesadas 125000 filas...\n",
      "Procesadas 126000 filas...\n",
      "Procesadas 127000 filas...\n",
      "Procesadas 128000 filas...\n",
      "Procesadas 129000 filas...\n",
      "Procesadas 130000 filas...\n",
      "Procesadas 131000 filas...\n",
      "Procesadas 132000 filas...\n",
      "Procesadas 133000 filas...\n",
      "Procesadas 134000 filas...\n",
      "Procesadas 135000 filas...\n",
      "Procesadas 136000 filas...\n",
      "Procesadas 137000 filas...\n",
      "Procesadas 138000 filas...\n",
      "Procesadas 139000 filas...\n",
      "Procesadas 140000 filas...\n",
      "Procesadas 141000 filas...\n",
      "Procesadas 142000 filas...\n",
      "Procesadas 143000 filas...\n",
      "Procesadas 144000 filas...\n",
      "Procesadas 145000 filas...\n",
      "Procesadas 146000 filas...\n",
      "Procesadas 147000 filas...\n",
      "Procesadas 148000 filas...\n",
      "Procesadas 149000 filas...\n",
      "Procesadas 150000 filas...\n",
      "Procesadas 151000 filas...\n",
      "Procesadas 152000 filas...\n",
      "Procesadas 153000 filas...\n",
      "Procesadas 154000 filas...\n",
      "Procesadas 155000 filas...\n",
      "Procesadas 156000 filas...\n",
      "Procesadas 157000 filas...\n",
      "Procesadas 158000 filas...\n",
      "Procesadas 159000 filas...\n",
      "Procesadas 160000 filas...\n",
      "Procesadas 161000 filas...\n",
      "Procesadas 162000 filas...\n",
      "Procesadas 163000 filas...\n",
      "Procesadas 164000 filas...\n",
      "Procesadas 165000 filas...\n",
      "Procesadas 166000 filas...\n",
      "Procesadas 167000 filas...\n",
      "Procesadas 168000 filas...\n",
      "Procesadas 169000 filas...\n",
      "Procesadas 170000 filas...\n",
      "Procesadas 171000 filas...\n",
      "Procesadas 172000 filas...\n",
      "Procesadas 172596 filas...\n",
      "Proceso finalizado: No hay más datos para descargar.\n",
      "Conexión cerrada.\n"
     ]
    }
   ],
   "source": [
    "# URL base de la API\n",
    "# URL sin paginación - la paginación se hará construyendo dinámicamente el query\n",
    "base_url_api = \"https://www.datos.gov.co/resource/kiw7-v9ta.csv\"\n",
    "\n",
    "# Parámetros\n",
    "limit = 1000  # número máximo permitido por la API\n",
    "offset = 0    # desplazamiento inicial\n",
    "all_data = [] # lista para almacenar los bloques\n",
    "\n",
    "# Configuración de los parámetros de conexión\n",
    "server = 'localhost\\\\SQLEXPRESS'  # O el punto '.' que usaste en SSMS\n",
    "database = 'EM_CUN'\n",
    "\n",
    "# Cadena de conexión usando Autenticación de Windows (Trusted_Connection)\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO dbo.dir_viento (\n",
    "        codigo_estacion, codigo_sensor, fecha_observacion, valor_observado,\n",
    "        nombre_estacion, departamento, municipio, zona_hidrografica,\n",
    "        latitud, longitud, descripcion_sensor, unidad_medida\n",
    "    ) \n",
    "    VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # 1. Establecer conexión inicial\n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.fast_executemany = True\n",
    "        \n",
    "        print(\"Iniciando proceso de descarga e ingesta...\")\n",
    "\n",
    "        while True:\n",
    "            # 2. Construir el query SoQL con LIMIT y OFFSET incluidos\n",
    "            soql_query = f\"\"\"SELECT\n",
    "  `codigoestacion`,\n",
    "  `codigosensor`,\n",
    "  `fechaobservacion`,\n",
    "  `valorobservado`,\n",
    "  `nombreestacion`,\n",
    "  `departamento`,\n",
    "  `municipio`,\n",
    "  `zonahidrografica`,\n",
    "  `latitud`,\n",
    "  `longitud`,\n",
    "  `descripcionsensor`,\n",
    "  `unidadmedida`\n",
    "WHERE\n",
    "  `fechaobservacion`\n",
    "    BETWEEN \"2025-12-01T00:00:00\" :: floating_timestamp\n",
    "    AND \"2025-12-31T23:59:59\" :: floating_timestamp\n",
    "  AND caseless_eq(`departamento`, \"CUNDINAMARCA\")\n",
    "LIMIT {limit}\n",
    "OFFSET {offset}\"\"\"\n",
    "            \n",
    "            # URL-encode el query\n",
    "            import urllib.parse\n",
    "            encoded_query = urllib.parse.quote(soql_query)\n",
    "            url = f\"{base_url_api}?$query={encoded_query}\"\n",
    "            \n",
    "            response = requests.get(url, timeout=15)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error en la API: {response.status_code}\")\n",
    "                print(f\"Respuesta: {response.text}\")\n",
    "                break\n",
    "\n",
    "            # 3. Leer bloque como DataFrame\n",
    "            df_chunk = pd.read_csv(StringIO(response.text), dtype=str)\n",
    "            \n",
    "            if df_chunk.empty:\n",
    "                print(\"Proceso finalizado: No hay más datos para descargar.\")\n",
    "                break\n",
    "            \n",
    "            # Convertir tipos de datos\n",
    "            df_chunk[\"valorobservado\"] = df_chunk[\"valorobservado\"].astype(float)\n",
    "            df_chunk[\"latitud\"] = df_chunk[\"latitud\"].astype(float)\n",
    "            df_chunk[\"longitud\"] = df_chunk[\"longitud\"].astype(float)  \n",
    "            df_chunk[\"fechaobservacion\"] = pd.to_datetime(df_chunk[\"fechaobservacion\"])            \n",
    "            \n",
    "            # 4. Cargar bloque actual a la base de datos\n",
    "            try:\n",
    "                records = df_chunk.values.tolist()\n",
    "                cursor.executemany(insert_query, records)\n",
    "                conn.commit()  # Commit por bloque para asegurar persistencia\n",
    "                \n",
    "                print(f\"Procesadas {offset + len(df_chunk)} filas...\")\n",
    "                \n",
    "            except Exception as e_db:\n",
    "                print(f\"Error insertando bloque en offset {offset}: {e_db}\")\n",
    "                conn.rollback()\n",
    "                break\n",
    "\n",
    "            # 5. Incrementar offset para el siguiente bloque\n",
    "            offset += limit\n",
    "\n",
    "except pyodbc.Error as e_conn:\n",
    "    print(f\"Error de conexión a la base de datos: {e_conn}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Conexión cerrada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181a94bb",
   "metadata": {},
   "source": [
    "## Velocidad del viento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55103b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de descarga e ingesta...\n",
      "Procesadas 1000 filas...\n",
      "Procesadas 2000 filas...\n",
      "Procesadas 3000 filas...\n",
      "Procesadas 4000 filas...\n",
      "Procesadas 5000 filas...\n",
      "Procesadas 6000 filas...\n",
      "Procesadas 7000 filas...\n",
      "Procesadas 8000 filas...\n",
      "Procesadas 9000 filas...\n",
      "Procesadas 10000 filas...\n",
      "Procesadas 11000 filas...\n",
      "Procesadas 12000 filas...\n",
      "Procesadas 13000 filas...\n",
      "Procesadas 14000 filas...\n",
      "Procesadas 15000 filas...\n",
      "Procesadas 16000 filas...\n",
      "Procesadas 17000 filas...\n",
      "Procesadas 18000 filas...\n",
      "Procesadas 19000 filas...\n",
      "Procesadas 20000 filas...\n",
      "Procesadas 21000 filas...\n",
      "Procesadas 22000 filas...\n",
      "Procesadas 23000 filas...\n",
      "Procesadas 24000 filas...\n",
      "Procesadas 25000 filas...\n",
      "Procesadas 26000 filas...\n",
      "Procesadas 27000 filas...\n",
      "Procesadas 28000 filas...\n",
      "Procesadas 29000 filas...\n",
      "Procesadas 30000 filas...\n",
      "Procesadas 31000 filas...\n",
      "Procesadas 32000 filas...\n",
      "Procesadas 33000 filas...\n",
      "Procesadas 34000 filas...\n",
      "Procesadas 35000 filas...\n",
      "Procesadas 36000 filas...\n",
      "Procesadas 37000 filas...\n",
      "Procesadas 38000 filas...\n",
      "Procesadas 39000 filas...\n",
      "Procesadas 40000 filas...\n",
      "Procesadas 41000 filas...\n",
      "Procesadas 42000 filas...\n",
      "Procesadas 43000 filas...\n",
      "Procesadas 44000 filas...\n",
      "Procesadas 45000 filas...\n",
      "Procesadas 46000 filas...\n",
      "Procesadas 47000 filas...\n",
      "Procesadas 48000 filas...\n",
      "Procesadas 49000 filas...\n",
      "Procesadas 50000 filas...\n",
      "Procesadas 51000 filas...\n",
      "Procesadas 52000 filas...\n",
      "Procesadas 53000 filas...\n",
      "Procesadas 54000 filas...\n",
      "Procesadas 55000 filas...\n",
      "Procesadas 56000 filas...\n",
      "Procesadas 57000 filas...\n",
      "Procesadas 58000 filas...\n",
      "Procesadas 59000 filas...\n",
      "Procesadas 60000 filas...\n",
      "Procesadas 61000 filas...\n",
      "Procesadas 62000 filas...\n",
      "Procesadas 63000 filas...\n",
      "Procesadas 64000 filas...\n",
      "Procesadas 65000 filas...\n",
      "Procesadas 66000 filas...\n",
      "Procesadas 67000 filas...\n",
      "Procesadas 68000 filas...\n",
      "Procesadas 69000 filas...\n",
      "Procesadas 70000 filas...\n",
      "Procesadas 71000 filas...\n",
      "Procesadas 72000 filas...\n",
      "Procesadas 73000 filas...\n",
      "Procesadas 74000 filas...\n",
      "Procesadas 75000 filas...\n",
      "Procesadas 76000 filas...\n",
      "Procesadas 77000 filas...\n",
      "Procesadas 78000 filas...\n",
      "Procesadas 79000 filas...\n",
      "Procesadas 80000 filas...\n",
      "Procesadas 81000 filas...\n",
      "Procesadas 82000 filas...\n",
      "Procesadas 83000 filas...\n",
      "Procesadas 84000 filas...\n",
      "Procesadas 85000 filas...\n",
      "Procesadas 86000 filas...\n",
      "Procesadas 87000 filas...\n",
      "Procesadas 88000 filas...\n",
      "Procesadas 89000 filas...\n",
      "Procesadas 90000 filas...\n",
      "Procesadas 91000 filas...\n",
      "Procesadas 92000 filas...\n",
      "Procesadas 93000 filas...\n",
      "Procesadas 94000 filas...\n",
      "Procesadas 95000 filas...\n",
      "Procesadas 96000 filas...\n",
      "Procesadas 97000 filas...\n",
      "Procesadas 98000 filas...\n",
      "Procesadas 99000 filas...\n",
      "Procesadas 100000 filas...\n",
      "Procesadas 101000 filas...\n",
      "Procesadas 102000 filas...\n",
      "Procesadas 103000 filas...\n",
      "Procesadas 104000 filas...\n",
      "Procesadas 105000 filas...\n",
      "Procesadas 106000 filas...\n",
      "Procesadas 107000 filas...\n",
      "Procesadas 108000 filas...\n",
      "Procesadas 109000 filas...\n",
      "Procesadas 110000 filas...\n",
      "Procesadas 111000 filas...\n",
      "Procesadas 112000 filas...\n",
      "Procesadas 113000 filas...\n",
      "Procesadas 114000 filas...\n",
      "Procesadas 115000 filas...\n",
      "Procesadas 116000 filas...\n",
      "Procesadas 117000 filas...\n",
      "Procesadas 118000 filas...\n",
      "Procesadas 119000 filas...\n",
      "Procesadas 120000 filas...\n",
      "Procesadas 121000 filas...\n",
      "Procesadas 122000 filas...\n",
      "Procesadas 123000 filas...\n",
      "Procesadas 124000 filas...\n",
      "Procesadas 125000 filas...\n",
      "Procesadas 126000 filas...\n",
      "Procesadas 127000 filas...\n",
      "Procesadas 128000 filas...\n",
      "Procesadas 129000 filas...\n",
      "Procesadas 130000 filas...\n",
      "Procesadas 131000 filas...\n",
      "Procesadas 132000 filas...\n",
      "Procesadas 133000 filas...\n",
      "Procesadas 134000 filas...\n",
      "Procesadas 135000 filas...\n",
      "Procesadas 136000 filas...\n",
      "Procesadas 137000 filas...\n",
      "Procesadas 138000 filas...\n",
      "Procesadas 139000 filas...\n",
      "Procesadas 140000 filas...\n",
      "Procesadas 141000 filas...\n",
      "Procesadas 142000 filas...\n",
      "Procesadas 143000 filas...\n",
      "Procesadas 144000 filas...\n",
      "Procesadas 145000 filas...\n",
      "Procesadas 146000 filas...\n",
      "Procesadas 147000 filas...\n",
      "Procesadas 148000 filas...\n",
      "Procesadas 149000 filas...\n",
      "Procesadas 150000 filas...\n",
      "Procesadas 151000 filas...\n",
      "Procesadas 152000 filas...\n",
      "Procesadas 153000 filas...\n",
      "Procesadas 154000 filas...\n",
      "Procesadas 155000 filas...\n",
      "Procesadas 156000 filas...\n",
      "Procesadas 157000 filas...\n",
      "Procesadas 158000 filas...\n",
      "Procesadas 159000 filas...\n",
      "Procesadas 160000 filas...\n",
      "Procesadas 161000 filas...\n",
      "Procesadas 162000 filas...\n",
      "Procesadas 163000 filas...\n",
      "Procesadas 164000 filas...\n",
      "Procesadas 165000 filas...\n",
      "Procesadas 166000 filas...\n",
      "Procesadas 167000 filas...\n",
      "Procesadas 168000 filas...\n",
      "Procesadas 169000 filas...\n",
      "Procesadas 170000 filas...\n",
      "Procesadas 171000 filas...\n",
      "Procesadas 172000 filas...\n",
      "Procesadas 172482 filas...\n",
      "Proceso finalizado: No hay más datos para descargar.\n",
      "Conexión cerrada.\n"
     ]
    }
   ],
   "source": [
    "# URL base de la API\n",
    "# URL sin paginación - la paginación se hará construyendo dinámicamente el query\n",
    "base_url_api = \"https://www.datos.gov.co/resource/sgfv-3yp8.csv\"\n",
    "\n",
    "# Parámetros\n",
    "limit = 1000  # número máximo permitido por la API\n",
    "offset = 0    # desplazamiento inicial\n",
    "all_data = [] # lista para almacenar los bloques\n",
    "\n",
    "# Configuración de los parámetros de conexión\n",
    "server = 'localhost\\\\SQLEXPRESS'  # O el punto '.' que usaste en SSMS\n",
    "database = 'EM_CUN'\n",
    "\n",
    "# Cadena de conexión usando Autenticación de Windows (Trusted_Connection)\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO dbo.vel_viento (\n",
    "        codigo_estacion, codigo_sensor, fecha_observacion, valor_observado,\n",
    "        nombre_estacion, departamento, municipio, zona_hidrografica,\n",
    "        latitud, longitud, descripcion_sensor, unidad_medida\n",
    "    ) \n",
    "    VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # 1. Establecer conexión inicial\n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.fast_executemany = True\n",
    "        \n",
    "        print(\"Iniciando proceso de descarga e ingesta...\")\n",
    "\n",
    "        while True:\n",
    "            # 2. Construir el query SoQL con LIMIT y OFFSET incluidos\n",
    "            soql_query = f\"\"\"SELECT\n",
    "  `codigoestacion`,\n",
    "  `codigosensor`,\n",
    "  `fechaobservacion`,\n",
    "  `valorobservado`,\n",
    "  `nombreestacion`,\n",
    "  `departamento`,\n",
    "  `municipio`,\n",
    "  `zonahidrografica`,\n",
    "  `latitud`,\n",
    "  `longitud`,\n",
    "  `descripcionsensor`,\n",
    "  `unidadmedida`\n",
    "WHERE\n",
    "  `fechaobservacion`\n",
    "    BETWEEN \"2025-12-01T00:00:00\" :: floating_timestamp\n",
    "    AND \"2025-12-31T23:59:59\" :: floating_timestamp\n",
    "  AND caseless_eq(`departamento`, \"CUNDINAMARCA\")\n",
    "LIMIT {limit}\n",
    "OFFSET {offset}\"\"\"\n",
    "            \n",
    "            # URL-encode el query\n",
    "            import urllib.parse\n",
    "            encoded_query = urllib.parse.quote(soql_query)\n",
    "            url = f\"{base_url_api}?$query={encoded_query}\"\n",
    "            \n",
    "            response = requests.get(url, timeout=15)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error en la API: {response.status_code}\")\n",
    "                print(f\"Respuesta: {response.text}\")\n",
    "                break\n",
    "\n",
    "            # 3. Leer bloque como DataFrame\n",
    "            df_chunk = pd.read_csv(StringIO(response.text), dtype=str)\n",
    "            \n",
    "            if df_chunk.empty:\n",
    "                print(\"Proceso finalizado: No hay más datos para descargar.\")\n",
    "                break\n",
    "            \n",
    "            # Convertir tipos de datos\n",
    "            df_chunk[\"valorobservado\"] = df_chunk[\"valorobservado\"].astype(float)\n",
    "            df_chunk[\"latitud\"] = df_chunk[\"latitud\"].astype(float)\n",
    "            df_chunk[\"longitud\"] = df_chunk[\"longitud\"].astype(float)  \n",
    "            df_chunk[\"fechaobservacion\"] = pd.to_datetime(df_chunk[\"fechaobservacion\"])            \n",
    "            \n",
    "            # 4. Cargar bloque actual a la base de datos\n",
    "            try:\n",
    "                records = df_chunk.values.tolist()\n",
    "                cursor.executemany(insert_query, records)\n",
    "                conn.commit()  # Commit por bloque para asegurar persistencia\n",
    "                \n",
    "                print(f\"Procesadas {offset + len(df_chunk)} filas...\")\n",
    "                \n",
    "            except Exception as e_db:\n",
    "                print(f\"Error insertando bloque en offset {offset}: {e_db}\")\n",
    "                conn.rollback()\n",
    "                break\n",
    "\n",
    "            # 5. Incrementar offset para el siguiente bloque\n",
    "            offset += limit\n",
    "\n",
    "except pyodbc.Error as e_conn:\n",
    "    print(f\"Error de conexión a la base de datos: {e_conn}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Conexión cerrada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fdbe50",
   "metadata": {},
   "source": [
    "## Temperatura del aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ce53e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de descarga e ingesta...\n",
      "Procesadas 1000 filas...\n",
      "Procesadas 2000 filas...\n",
      "Procesadas 3000 filas...\n",
      "Procesadas 4000 filas...\n",
      "Procesadas 5000 filas...\n",
      "Procesadas 6000 filas...\n",
      "Procesadas 7000 filas...\n",
      "Procesadas 8000 filas...\n",
      "Procesadas 9000 filas...\n",
      "Procesadas 10000 filas...\n",
      "Procesadas 11000 filas...\n",
      "Procesadas 12000 filas...\n",
      "Procesadas 13000 filas...\n",
      "Procesadas 14000 filas...\n",
      "Procesadas 15000 filas...\n",
      "Procesadas 16000 filas...\n",
      "Procesadas 17000 filas...\n",
      "Procesadas 18000 filas...\n",
      "Procesadas 19000 filas...\n",
      "Procesadas 20000 filas...\n",
      "Procesadas 21000 filas...\n",
      "Procesadas 22000 filas...\n",
      "Procesadas 23000 filas...\n",
      "Procesadas 24000 filas...\n",
      "Procesadas 25000 filas...\n",
      "Procesadas 26000 filas...\n",
      "Procesadas 27000 filas...\n",
      "Procesadas 28000 filas...\n",
      "Procesadas 29000 filas...\n",
      "Procesadas 30000 filas...\n",
      "Procesadas 31000 filas...\n",
      "Procesadas 32000 filas...\n",
      "Procesadas 33000 filas...\n",
      "Procesadas 34000 filas...\n",
      "Procesadas 35000 filas...\n",
      "Procesadas 36000 filas...\n",
      "Procesadas 37000 filas...\n",
      "Procesadas 38000 filas...\n",
      "Procesadas 39000 filas...\n",
      "Procesadas 40000 filas...\n",
      "Procesadas 41000 filas...\n",
      "Procesadas 42000 filas...\n",
      "Procesadas 43000 filas...\n",
      "Procesadas 44000 filas...\n",
      "Procesadas 45000 filas...\n",
      "Procesadas 46000 filas...\n",
      "Procesadas 47000 filas...\n",
      "Procesadas 48000 filas...\n",
      "Procesadas 49000 filas...\n",
      "Procesadas 50000 filas...\n",
      "Procesadas 51000 filas...\n",
      "Procesadas 52000 filas...\n",
      "Procesadas 53000 filas...\n",
      "Procesadas 54000 filas...\n",
      "Procesadas 55000 filas...\n",
      "Procesadas 56000 filas...\n",
      "Procesadas 57000 filas...\n",
      "Procesadas 58000 filas...\n",
      "Procesadas 59000 filas...\n",
      "Procesadas 60000 filas...\n",
      "Procesadas 61000 filas...\n",
      "Procesadas 62000 filas...\n",
      "Procesadas 63000 filas...\n",
      "Procesadas 64000 filas...\n",
      "Procesadas 65000 filas...\n",
      "Procesadas 66000 filas...\n",
      "Procesadas 67000 filas...\n",
      "Procesadas 68000 filas...\n",
      "Procesadas 69000 filas...\n",
      "Procesadas 70000 filas...\n",
      "Procesadas 71000 filas...\n",
      "Procesadas 72000 filas...\n",
      "Procesadas 73000 filas...\n",
      "Procesadas 74000 filas...\n",
      "Procesadas 75000 filas...\n",
      "Procesadas 76000 filas...\n",
      "Procesadas 77000 filas...\n",
      "Procesadas 78000 filas...\n",
      "Procesadas 79000 filas...\n",
      "Procesadas 80000 filas...\n",
      "Procesadas 80065 filas...\n",
      "Proceso finalizado: No hay más datos para descargar.\n",
      "Conexión cerrada.\n"
     ]
    }
   ],
   "source": [
    "# URL base de la API\n",
    "# URL sin paginación - la paginación se hará construyendo dinámicamente el query\n",
    "base_url_api = \"https://www.datos.gov.co/resource/sbwg-7ju4.csv\"\n",
    "\n",
    "# Parámetros\n",
    "limit = 1000  # número máximo permitido por la API\n",
    "offset = 0    # desplazamiento inicial\n",
    "all_data = [] # lista para almacenar los bloques\n",
    "\n",
    "# Configuración de los parámetros de conexión\n",
    "server = 'localhost\\\\SQLEXPRESS'  # O el punto '.' que usaste en SSMS\n",
    "database = 'EM_CUN'\n",
    "\n",
    "# Cadena de conexión usando Autenticación de Windows (Trusted_Connection)\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO dbo.temp_aire (\n",
    "        codigo_estacion, codigo_sensor, fecha_observacion, valor_observado,\n",
    "        nombre_estacion, departamento, municipio, zona_hidrografica,\n",
    "        latitud, longitud, descripcion_sensor, unidad_medida\n",
    "    ) \n",
    "    VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # 1. Establecer conexión inicial\n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.fast_executemany = True\n",
    "        \n",
    "        print(\"Iniciando proceso de descarga e ingesta...\")\n",
    "\n",
    "        while True:\n",
    "            # 2. Construir el query SoQL con LIMIT y OFFSET incluidos\n",
    "            soql_query = f\"\"\"SELECT\n",
    "  `codigoestacion`,\n",
    "  `codigosensor`,\n",
    "  `fechaobservacion`,\n",
    "  `valorobservado`,\n",
    "  `nombreestacion`,\n",
    "  `departamento`,\n",
    "  `municipio`,\n",
    "  `zonahidrografica`,\n",
    "  `latitud`,\n",
    "  `longitud`,\n",
    "  `descripcionsensor`,\n",
    "  `unidadmedida`\n",
    "WHERE\n",
    "  `fechaobservacion`\n",
    "    BETWEEN \"2025-12-01T00:00:00\" :: floating_timestamp\n",
    "    AND \"2025-12-31T23:59:59\" :: floating_timestamp\n",
    "  AND caseless_eq(`departamento`, \"CUNDINAMARCA\")\n",
    "LIMIT {limit}\n",
    "OFFSET {offset}\"\"\"\n",
    "            \n",
    "            # URL-encode el query\n",
    "            import urllib.parse\n",
    "            encoded_query = urllib.parse.quote(soql_query)\n",
    "            url = f\"{base_url_api}?$query={encoded_query}\"\n",
    "            \n",
    "            response = requests.get(url, timeout=15)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error en la API: {response.status_code}\")\n",
    "                print(f\"Respuesta: {response.text}\")\n",
    "                break\n",
    "\n",
    "            # 3. Leer bloque como DataFrame\n",
    "            df_chunk = pd.read_csv(StringIO(response.text), dtype=str)\n",
    "            \n",
    "            if df_chunk.empty:\n",
    "                print(\"Proceso finalizado: No hay más datos para descargar.\")\n",
    "                break\n",
    "            \n",
    "            # Convertir tipos de datos\n",
    "            df_chunk[\"valorobservado\"] = df_chunk[\"valorobservado\"].astype(float)\n",
    "            df_chunk[\"latitud\"] = df_chunk[\"latitud\"].astype(float)\n",
    "            df_chunk[\"longitud\"] = df_chunk[\"longitud\"].astype(float)  \n",
    "            df_chunk[\"fechaobservacion\"] = pd.to_datetime(df_chunk[\"fechaobservacion\"])            \n",
    "            \n",
    "            # 4. Cargar bloque actual a la base de datos\n",
    "            try:\n",
    "                records = df_chunk.values.tolist()\n",
    "                cursor.executemany(insert_query, records)\n",
    "                conn.commit()  # Commit por bloque para asegurar persistencia\n",
    "                \n",
    "                print(f\"Procesadas {offset + len(df_chunk)} filas...\")\n",
    "                \n",
    "            except Exception as e_db:\n",
    "                print(f\"Error insertando bloque en offset {offset}: {e_db}\")\n",
    "                conn.rollback()\n",
    "                break\n",
    "\n",
    "            # 5. Incrementar offset para el siguiente bloque\n",
    "            offset += limit\n",
    "\n",
    "except pyodbc.Error as e_conn:\n",
    "    print(f\"Error de conexión a la base de datos: {e_conn}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Conexión cerrada.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
