{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f67788",
   "metadata": {},
   "source": [
    "Cuaderno de python para almacenar los datos extraidos de datos abiertos en una base en sql server para su posterior procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "666e9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pyodbc\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb4413",
   "metadata": {},
   "source": [
    "# Bogotá Dic 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f76218",
   "metadata": {},
   "source": [
    "## Presión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf12735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de descarga e ingesta...\n",
      "Procesadas 1000 filas...\n",
      "Procesadas 2000 filas...\n",
      "Procesadas 3000 filas...\n",
      "Procesadas 4000 filas...\n",
      "Procesadas 5000 filas...\n",
      "Procesadas 6000 filas...\n",
      "Procesadas 7000 filas...\n",
      "Procesadas 8000 filas...\n",
      "Procesadas 9000 filas...\n",
      "Procesadas 10000 filas...\n",
      "Procesadas 11000 filas...\n",
      "Procesadas 12000 filas...\n",
      "Procesadas 13000 filas...\n",
      "Procesadas 14000 filas...\n",
      "Procesadas 15000 filas...\n",
      "Procesadas 16000 filas...\n",
      "Procesadas 17000 filas...\n",
      "Procesadas 18000 filas...\n",
      "Procesadas 19000 filas...\n",
      "Procesadas 20000 filas...\n",
      "Procesadas 21000 filas...\n",
      "Procesadas 22000 filas...\n",
      "Procesadas 23000 filas...\n",
      "Procesadas 24000 filas...\n",
      "Procesadas 25000 filas...\n",
      "Procesadas 26000 filas...\n",
      "Procesadas 27000 filas...\n",
      "Procesadas 27699 filas...\n",
      "Proceso finalizado: No hay más datos para descargar.\n",
      "Conexión cerrada.\n"
     ]
    }
   ],
   "source": [
    "# URL base de la API\n",
    "# URL sin paginación - la paginación se hará construyendo dinámicamente el query\n",
    "base_url_api = \"https://www.datos.gov.co/resource/62tk-nxj5.csv\"\n",
    "\n",
    "# Parámetros\n",
    "limit = 1000  # Reducido a 500 para queries más rápidas\n",
    "offset = 0    # desplazamiento inicial\n",
    "all_data = [] # lista para almacenar los bloques\n",
    "\n",
    "# Configuración de los parámetros de conexión\n",
    "server = 'localhost\\\\SQLEXPRESS'  # O el punto '.' que usaste en SSMS\n",
    "database = 'EM_BOG'\n",
    "\n",
    "# Cadena de conexión usando Autenticación de Windows (Trusted_Connection)\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO dbo.presion (\n",
    "        codigo_estacion, codigo_sensor, fecha_observacion, valor_observado,\n",
    "        nombre_estacion, departamento, municipio, zona_hidrografica,\n",
    "        latitud, longitud, descripcion_sensor, unidad_medida\n",
    "    ) \n",
    "    VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "\n",
    "# Función para reintentos\n",
    "import time\n",
    "def request_with_retries(url, max_retries=3, timeout=60):\n",
    "    \"\"\"Intenta hacer la solicitud hasta 3 veces con timeout de 60 segundos\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout)\n",
    "            return response\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout en intento {attempt + 1}/{max_retries}. Esperando 5 segundos...\")\n",
    "            time.sleep(5)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"Error de conexión en intento {attempt + 1}/{max_retries}: {e}\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    raise Exception(f\"No se pudo conectar después de {max_retries} intentos\")\n",
    "\n",
    "try:\n",
    "    # 1. Establecer conexión inicial\n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.fast_executemany = True\n",
    "        \n",
    "        print(\"Iniciando proceso de descarga e ingesta...\")\n",
    "\n",
    "        while True:\n",
    "            # 2. Construir el query SoQL con LIMIT y OFFSET incluidos\n",
    "            soql_query = f\"\"\"SELECT\n",
    "  `codigoestacion`,\n",
    "  `codigosensor`,\n",
    "  `fechaobservacion`,\n",
    "  `valorobservado`,\n",
    "  `nombreestacion`,\n",
    "  `departamento`,\n",
    "  `municipio`,\n",
    "  `zonahidrografica`,\n",
    "  `latitud`,\n",
    "  `longitud`,\n",
    "  `descripcionsensor`,\n",
    "  `unidadmedida`\n",
    "WHERE\n",
    "  `fechaobservacion`\n",
    "    BETWEEN \"2025-12-01T00:00:00\" :: floating_timestamp\n",
    "    AND \"2025-12-31T23:59:59\" :: floating_timestamp\n",
    "  AND caseless_eq(`departamento`, \"BOGOTÁ\")\n",
    "LIMIT {limit}\n",
    "OFFSET {offset}\"\"\"\n",
    "            \n",
    "            # URL-encode el query\n",
    "            import urllib.parse\n",
    "            encoded_query = urllib.parse.quote(soql_query)\n",
    "            url = f\"{base_url_api}?$query={encoded_query}\"\n",
    "            \n",
    "            response = request_with_retries(url, max_retries=3, timeout=60)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error en la API: {response.status_code}\")\n",
    "                print(f\"Respuesta: {response.text}\")\n",
    "                break\n",
    "\n",
    "            # 3. Leer bloque como DataFrame\n",
    "            df_chunk = pd.read_csv(StringIO(response.text), dtype=str)\n",
    "            \n",
    "            if df_chunk.empty:\n",
    "                print(\"Proceso finalizado: No hay más datos para descargar.\")\n",
    "                break\n",
    "            \n",
    "            # Convertir tipos de datos\n",
    "            df_chunk[\"valorobservado\"] = df_chunk[\"valorobservado\"].astype(float)\n",
    "            df_chunk[\"latitud\"] = df_chunk[\"latitud\"].astype(float)\n",
    "            df_chunk[\"longitud\"] = df_chunk[\"longitud\"].astype(float)  \n",
    "            df_chunk[\"fechaobservacion\"] = pd.to_datetime(df_chunk[\"fechaobservacion\"])            \n",
    "            \n",
    "            # 4. Cargar bloque actual a la base de datos\n",
    "            try:\n",
    "                records = df_chunk.values.tolist()\n",
    "                cursor.executemany(insert_query, records)\n",
    "                conn.commit()  # Commit por bloque para asegurar persistencia\n",
    "                \n",
    "                print(f\"Procesadas {offset + len(df_chunk)} filas...\")\n",
    "                \n",
    "            except Exception as e_db:\n",
    "                print(f\"Error insertando bloque en offset {offset}: {e_db}\")\n",
    "                conn.rollback()\n",
    "                break\n",
    "\n",
    "            # 5. Incrementar offset para el siguiente bloque\n",
    "            offset += limit\n",
    "\n",
    "except pyodbc.Error as e_conn:\n",
    "    print(f\"Error de conexión a la base de datos: {e_conn}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Conexión cerrada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b725c8dd",
   "metadata": {},
   "source": [
    "## Dirección del viento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3468171a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de descarga e ingesta...\n",
      "Procesadas 1000 filas...\n",
      "Procesadas 2000 filas...\n",
      "Procesadas 3000 filas...\n",
      "Procesadas 4000 filas...\n",
      "Procesadas 5000 filas...\n",
      "Procesadas 6000 filas...\n",
      "Procesadas 7000 filas...\n",
      "Procesadas 8000 filas...\n",
      "Procesadas 9000 filas...\n",
      "Procesadas 10000 filas...\n",
      "Procesadas 11000 filas...\n",
      "Procesadas 12000 filas...\n",
      "Procesadas 13000 filas...\n",
      "Procesadas 14000 filas...\n",
      "Procesadas 15000 filas...\n",
      "Procesadas 16000 filas...\n",
      "Procesadas 17000 filas...\n",
      "Procesadas 18000 filas...\n",
      "Procesadas 19000 filas...\n",
      "Procesadas 20000 filas...\n",
      "Procesadas 21000 filas...\n",
      "Procesadas 22000 filas...\n",
      "Procesadas 23000 filas...\n",
      "Procesadas 24000 filas...\n",
      "Procesadas 25000 filas...\n",
      "Procesadas 26000 filas...\n",
      "Procesadas 27000 filas...\n",
      "Procesadas 28000 filas...\n",
      "Procesadas 29000 filas...\n",
      "Procesadas 30000 filas...\n",
      "Procesadas 31000 filas...\n",
      "Procesadas 32000 filas...\n",
      "Procesadas 32421 filas...\n",
      "Proceso finalizado: No hay más datos para descargar.\n",
      "Conexión cerrada.\n"
     ]
    }
   ],
   "source": [
    "# URL base de la API\n",
    "# URL sin paginación - la paginación se hará construyendo dinámicamente el query\n",
    "base_url_api = \"https://www.datos.gov.co/resource/kiw7-v9ta.csv\"\n",
    "\n",
    "# Parámetros\n",
    "limit = 1000  # número máximo permitido por la API\n",
    "offset = 0    # desplazamiento inicial\n",
    "all_data = [] # lista para almacenar los bloques\n",
    "\n",
    "# Configuración de los parámetros de conexión\n",
    "server = 'localhost\\\\SQLEXPRESS'  # O el punto '.' que usaste en SSMS\n",
    "database = 'EM_BOG'\n",
    "\n",
    "# Cadena de conexión usando Autenticación de Windows (Trusted_Connection)\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO dbo.dir_viento (\n",
    "        codigo_estacion, codigo_sensor, fecha_observacion, valor_observado,\n",
    "        nombre_estacion, departamento, municipio, zona_hidrografica,\n",
    "        latitud, longitud, descripcion_sensor, unidad_medida\n",
    "    ) \n",
    "    VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # 1. Establecer conexión inicial\n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.fast_executemany = True\n",
    "        \n",
    "        print(\"Iniciando proceso de descarga e ingesta...\")\n",
    "\n",
    "        while True:\n",
    "            # 2. Construir el query SoQL con LIMIT y OFFSET incluidos\n",
    "            soql_query = f\"\"\"SELECT\n",
    "  `codigoestacion`,\n",
    "  `codigosensor`,\n",
    "  `fechaobservacion`,\n",
    "  `valorobservado`,\n",
    "  `nombreestacion`,\n",
    "  `departamento`,\n",
    "  `municipio`,\n",
    "  `zonahidrografica`,\n",
    "  `latitud`,\n",
    "  `longitud`,\n",
    "  `descripcionsensor`,\n",
    "  `unidadmedida`\n",
    "WHERE\n",
    "  `fechaobservacion`\n",
    "    BETWEEN \"2025-12-01T00:00:00\" :: floating_timestamp\n",
    "    AND \"2025-12-31T23:59:59\" :: floating_timestamp\n",
    "  AND caseless_eq(`departamento`, \"BOGOTÁ\")\n",
    "LIMIT {limit}\n",
    "OFFSET {offset}\"\"\"\n",
    "            \n",
    "            # URL-encode el query\n",
    "            import urllib.parse\n",
    "            encoded_query = urllib.parse.quote(soql_query)\n",
    "            url = f\"{base_url_api}?$query={encoded_query}\"\n",
    "            \n",
    "            response = requests.get(url, timeout=15)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error en la API: {response.status_code}\")\n",
    "                print(f\"Respuesta: {response.text}\")\n",
    "                break\n",
    "\n",
    "            # 3. Leer bloque como DataFrame\n",
    "            df_chunk = pd.read_csv(StringIO(response.text), dtype=str)\n",
    "            \n",
    "            if df_chunk.empty:\n",
    "                print(\"Proceso finalizado: No hay más datos para descargar.\")\n",
    "                break\n",
    "            \n",
    "            # Convertir tipos de datos\n",
    "            df_chunk[\"valorobservado\"] = df_chunk[\"valorobservado\"].astype(float)\n",
    "            df_chunk[\"latitud\"] = df_chunk[\"latitud\"].astype(float)\n",
    "            df_chunk[\"longitud\"] = df_chunk[\"longitud\"].astype(float)  \n",
    "            df_chunk[\"fechaobservacion\"] = pd.to_datetime(df_chunk[\"fechaobservacion\"])            \n",
    "            \n",
    "            # 4. Cargar bloque actual a la base de datos\n",
    "            try:\n",
    "                records = df_chunk.values.tolist()\n",
    "                cursor.executemany(insert_query, records)\n",
    "                conn.commit()  # Commit por bloque para asegurar persistencia\n",
    "                \n",
    "                print(f\"Procesadas {offset + len(df_chunk)} filas...\")\n",
    "                \n",
    "            except Exception as e_db:\n",
    "                print(f\"Error insertando bloque en offset {offset}: {e_db}\")\n",
    "                conn.rollback()\n",
    "                break\n",
    "\n",
    "            # 5. Incrementar offset para el siguiente bloque\n",
    "            offset += limit\n",
    "\n",
    "except pyodbc.Error as e_conn:\n",
    "    print(f\"Error de conexión a la base de datos: {e_conn}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Conexión cerrada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b0846",
   "metadata": {},
   "source": [
    "## Velocidad del viento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8db60d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de descarga e ingesta...\n",
      "Procesadas 1000 filas...\n",
      "Procesadas 2000 filas...\n",
      "Procesadas 3000 filas...\n",
      "Procesadas 4000 filas...\n",
      "Procesadas 5000 filas...\n",
      "Procesadas 6000 filas...\n",
      "Procesadas 7000 filas...\n",
      "Procesadas 8000 filas...\n",
      "Procesadas 9000 filas...\n",
      "Procesadas 10000 filas...\n",
      "Procesadas 11000 filas...\n",
      "Procesadas 12000 filas...\n",
      "Procesadas 13000 filas...\n",
      "Procesadas 14000 filas...\n",
      "Procesadas 15000 filas...\n",
      "Procesadas 16000 filas...\n",
      "Procesadas 17000 filas...\n",
      "Procesadas 18000 filas...\n",
      "Procesadas 19000 filas...\n",
      "Procesadas 20000 filas...\n",
      "Procesadas 21000 filas...\n",
      "Procesadas 22000 filas...\n",
      "Procesadas 23000 filas...\n",
      "Procesadas 24000 filas...\n",
      "Procesadas 25000 filas...\n",
      "Procesadas 26000 filas...\n",
      "Procesadas 27000 filas...\n",
      "Procesadas 28000 filas...\n",
      "Procesadas 29000 filas...\n",
      "Procesadas 30000 filas...\n",
      "Procesadas 31000 filas...\n",
      "Procesadas 32000 filas...\n",
      "Procesadas 33000 filas...\n",
      "Procesadas 34000 filas...\n",
      "Procesadas 35000 filas...\n",
      "Procesadas 36000 filas...\n",
      "Procesadas 37000 filas...\n",
      "Procesadas 38000 filas...\n",
      "Procesadas 39000 filas...\n",
      "Procesadas 40000 filas...\n",
      "Procesadas 41000 filas...\n",
      "Procesadas 42000 filas...\n",
      "Procesadas 43000 filas...\n",
      "Procesadas 44000 filas...\n",
      "Procesadas 45000 filas...\n",
      "Procesadas 46000 filas...\n",
      "Procesadas 47000 filas...\n",
      "Procesadas 48000 filas...\n",
      "Procesadas 49000 filas...\n",
      "Procesadas 50000 filas...\n",
      "Procesadas 51000 filas...\n",
      "Procesadas 52000 filas...\n",
      "Procesadas 53000 filas...\n",
      "Procesadas 54000 filas...\n",
      "Procesadas 54146 filas...\n",
      "Proceso finalizado: No hay más datos para descargar.\n",
      "Conexión cerrada.\n"
     ]
    }
   ],
   "source": [
    "# URL base de la API\n",
    "# URL sin paginación - la paginación se hará construyendo dinámicamente el query\n",
    "base_url_api = \"https://www.datos.gov.co/resource/sgfv-3yp8.csv\"\n",
    "\n",
    "# Parámetros\n",
    "limit = 1000  # número máximo permitido por la API\n",
    "offset = 0    # desplazamiento inicial\n",
    "all_data = [] # lista para almacenar los bloques\n",
    "\n",
    "# Configuración de los parámetros de conexión\n",
    "server = 'localhost\\\\SQLEXPRESS'  # O el punto '.' que usaste en SSMS\n",
    "database = 'EM_BOG'\n",
    "\n",
    "# Cadena de conexión usando Autenticación de Windows (Trusted_Connection)\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO dbo.vel_viento (\n",
    "        codigo_estacion, codigo_sensor, fecha_observacion, valor_observado,\n",
    "        nombre_estacion, departamento, municipio, zona_hidrografica,\n",
    "        latitud, longitud, descripcion_sensor, unidad_medida\n",
    "    ) \n",
    "    VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # 1. Establecer conexión inicial\n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.fast_executemany = True\n",
    "        \n",
    "        print(\"Iniciando proceso de descarga e ingesta...\")\n",
    "\n",
    "        while True:\n",
    "            # 2. Construir el query SoQL con LIMIT y OFFSET incluidos\n",
    "            soql_query = f\"\"\"SELECT\n",
    "  `codigoestacion`,\n",
    "  `codigosensor`,\n",
    "  `fechaobservacion`,\n",
    "  `valorobservado`,\n",
    "  `nombreestacion`,\n",
    "  `departamento`,\n",
    "  `municipio`,\n",
    "  `zonahidrografica`,\n",
    "  `latitud`,\n",
    "  `longitud`,\n",
    "  `descripcionsensor`,\n",
    "  `unidadmedida`\n",
    "WHERE\n",
    "  `fechaobservacion`\n",
    "    BETWEEN \"2025-12-01T00:00:00\" :: floating_timestamp\n",
    "    AND \"2025-12-31T23:59:59\" :: floating_timestamp\n",
    "  AND caseless_eq(`departamento`, \"BOGOTÁ\")\n",
    "\n",
    "LIMIT {limit}\n",
    "OFFSET {offset}\"\"\"\n",
    "            \n",
    "            # URL-encode el query\n",
    "            import urllib.parse\n",
    "            encoded_query = urllib.parse.quote(soql_query)\n",
    "            url = f\"{base_url_api}?$query={encoded_query}\"\n",
    "            \n",
    "            response = requests.get(url, timeout=15)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error en la API: {response.status_code}\")\n",
    "                print(f\"Respuesta: {response.text}\")\n",
    "                break\n",
    "\n",
    "            # 3. Leer bloque como DataFrame\n",
    "            df_chunk = pd.read_csv(StringIO(response.text), dtype=str)\n",
    "            \n",
    "            if df_chunk.empty:\n",
    "                print(\"Proceso finalizado: No hay más datos para descargar.\")\n",
    "                break\n",
    "            \n",
    "            # Convertir tipos de datos\n",
    "            df_chunk[\"valorobservado\"] = df_chunk[\"valorobservado\"].astype(float)\n",
    "            df_chunk[\"latitud\"] = df_chunk[\"latitud\"].astype(float)\n",
    "            df_chunk[\"longitud\"] = df_chunk[\"longitud\"].astype(float)  \n",
    "            df_chunk[\"fechaobservacion\"] = pd.to_datetime(df_chunk[\"fechaobservacion\"])            \n",
    "            \n",
    "            # 4. Cargar bloque actual a la base de datos\n",
    "            try:\n",
    "                records = df_chunk.values.tolist()\n",
    "                cursor.executemany(insert_query, records)\n",
    "                conn.commit()  # Commit por bloque para asegurar persistencia\n",
    "                \n",
    "                print(f\"Procesadas {offset + len(df_chunk)} filas...\")\n",
    "                \n",
    "            except Exception as e_db:\n",
    "                print(f\"Error insertando bloque en offset {offset}: {e_db}\")\n",
    "                conn.rollback()\n",
    "                break\n",
    "\n",
    "            # 5. Incrementar offset para el siguiente bloque\n",
    "            offset += limit\n",
    "\n",
    "except pyodbc.Error as e_conn:\n",
    "    print(f\"Error de conexión a la base de datos: {e_conn}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Conexión cerrada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7108b8a",
   "metadata": {},
   "source": [
    "## Temperatura del aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f9aa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de descarga e ingesta...\n",
      "Procesadas 1000 filas...\n",
      "Procesadas 2000 filas...\n",
      "Procesadas 3000 filas...\n",
      "Procesadas 4000 filas...\n",
      "Procesadas 5000 filas...\n",
      "Procesadas 6000 filas...\n",
      "Procesadas 7000 filas...\n",
      "Procesadas 8000 filas...\n",
      "Procesadas 9000 filas...\n",
      "Procesadas 10000 filas...\n",
      "Procesadas 11000 filas...\n",
      "Procesadas 12000 filas...\n",
      "Procesadas 13000 filas...\n",
      "Procesadas 14000 filas...\n",
      "Procesadas 15000 filas...\n",
      "Procesadas 16000 filas...\n",
      "Procesadas 17000 filas...\n",
      "Procesadas 18000 filas...\n",
      "Procesadas 19000 filas...\n",
      "Procesadas 20000 filas...\n",
      "Procesadas 21000 filas...\n",
      "Procesadas 22000 filas...\n",
      "Procesadas 23000 filas...\n",
      "Procesadas 24000 filas...\n",
      "Procesadas 25000 filas...\n",
      "Procesadas 26000 filas...\n",
      "Procesadas 27000 filas...\n",
      "Procesadas 28000 filas...\n",
      "Procesadas 28396 filas...\n",
      "Proceso finalizado: No hay más datos para descargar.\n",
      "Conexión cerrada.\n"
     ]
    }
   ],
   "source": [
    "# URL base de la API\n",
    "# URL sin paginación - la paginación se hará construyendo dinámicamente el query\n",
    "base_url_api = \"https://www.datos.gov.co/resource/sbwg-7ju4.csv\"\n",
    "\n",
    "# Parámetros\n",
    "limit = 1000  # número máximo permitido por la API\n",
    "offset = 0    # desplazamiento inicial\n",
    "all_data = [] # lista para almacenar los bloques\n",
    "\n",
    "# Configuración de los parámetros de conexión\n",
    "server = 'localhost\\\\SQLEXPRESS'  # O el punto '.' que usaste en SSMS\n",
    "database = 'EM_BOG'\n",
    "\n",
    "# Cadena de conexión usando Autenticación de Windows (Trusted_Connection)\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO dbo.temp_aire (\n",
    "        codigo_estacion, codigo_sensor, fecha_observacion, valor_observado,\n",
    "        nombre_estacion, departamento, municipio, zona_hidrografica,\n",
    "        latitud, longitud, descripcion_sensor, unidad_medida\n",
    "    ) \n",
    "    VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # 1. Establecer conexión inicial\n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.fast_executemany = True\n",
    "        \n",
    "        print(\"Iniciando proceso de descarga e ingesta...\")\n",
    "\n",
    "        while True:\n",
    "            # 2. Construir el query SoQL con LIMIT y OFFSET incluidos\n",
    "            soql_query = f\"\"\"SELECT\n",
    "  `codigoestacion`,\n",
    "  `codigosensor`,\n",
    "  `fechaobservacion`,\n",
    "  `valorobservado`,\n",
    "  `nombreestacion`,\n",
    "  `departamento`,\n",
    "  `municipio`,\n",
    "  `zonahidrografica`,\n",
    "  `latitud`,\n",
    "  `longitud`,\n",
    "  `descripcionsensor`,\n",
    "  `unidadmedida`\n",
    "WHERE\n",
    "  `fechaobservacion`\n",
    "    BETWEEN \"2025-12-01T00:00:00\" :: floating_timestamp\n",
    "    AND \"2025-12-31T23:59:59\" :: floating_timestamp\n",
    "  AND caseless_eq(`departamento`, \"BOGOTÁ\")\n",
    "\n",
    "LIMIT {limit}\n",
    "OFFSET {offset}\"\"\"\n",
    "            \n",
    "            # URL-encode el query\n",
    "            import urllib.parse\n",
    "            encoded_query = urllib.parse.quote(soql_query)\n",
    "            url = f\"{base_url_api}?$query={encoded_query}\"\n",
    "            \n",
    "            response = requests.get(url, timeout=15)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error en la API: {response.status_code}\")\n",
    "                print(f\"Respuesta: {response.text}\")\n",
    "                break\n",
    "\n",
    "            # 3. Leer bloque como DataFrame\n",
    "            df_chunk = pd.read_csv(StringIO(response.text), dtype=str)\n",
    "            \n",
    "            if df_chunk.empty:\n",
    "                print(\"Proceso finalizado: No hay más datos para descargar.\")\n",
    "                break\n",
    "            \n",
    "            # Convertir tipos de datos\n",
    "            df_chunk[\"valorobservado\"] = df_chunk[\"valorobservado\"].astype(float)\n",
    "            df_chunk[\"latitud\"] = df_chunk[\"latitud\"].astype(float)\n",
    "            df_chunk[\"longitud\"] = df_chunk[\"longitud\"].astype(float)  \n",
    "            df_chunk[\"fechaobservacion\"] = pd.to_datetime(df_chunk[\"fechaobservacion\"])            \n",
    "            \n",
    "            # 4. Cargar bloque actual a la base de datos\n",
    "            try:\n",
    "                records = df_chunk.values.tolist()\n",
    "                cursor.executemany(insert_query, records)\n",
    "                conn.commit()  # Commit por bloque para asegurar persistencia\n",
    "                \n",
    "                print(f\"Procesadas {offset + len(df_chunk)} filas...\")\n",
    "                \n",
    "            except Exception as e_db:\n",
    "                print(f\"Error insertando bloque en offset {offset}: {e_db}\")\n",
    "                conn.rollback()\n",
    "                break\n",
    "\n",
    "            # 5. Incrementar offset para el siguiente bloque\n",
    "            offset += limit\n",
    "\n",
    "except pyodbc.Error as e_conn:\n",
    "    print(f\"Error de conexión a la base de datos: {e_conn}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Conexión cerrada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d952f5bb",
   "metadata": {},
   "source": [
    "# Cundinamarca y Boyacá Dic 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaae7b1",
   "metadata": {},
   "source": [
    "## Presión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f400b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de descarga e ingesta...\n",
      "Procesadas 1000 filas...\n",
      "Procesadas 2000 filas...\n",
      "Procesadas 3000 filas...\n",
      "Procesadas 4000 filas...\n",
      "Procesadas 5000 filas...\n",
      "Procesadas 6000 filas...\n",
      "Procesadas 7000 filas...\n",
      "Procesadas 8000 filas...\n",
      "Procesadas 9000 filas...\n",
      "Procesadas 10000 filas...\n",
      "Procesadas 11000 filas...\n",
      "Procesadas 12000 filas...\n",
      "Procesadas 13000 filas...\n",
      "Procesadas 14000 filas...\n",
      "Procesadas 15000 filas...\n",
      "Procesadas 16000 filas...\n",
      "Procesadas 17000 filas...\n",
      "Procesadas 18000 filas...\n",
      "Procesadas 19000 filas...\n",
      "Procesadas 20000 filas...\n",
      "Procesadas 21000 filas...\n",
      "Procesadas 22000 filas...\n",
      "Procesadas 23000 filas...\n",
      "Procesadas 24000 filas...\n",
      "Procesadas 25000 filas...\n",
      "Procesadas 26000 filas...\n",
      "Procesadas 27000 filas...\n",
      "Procesadas 28000 filas...\n",
      "Procesadas 29000 filas...\n",
      "Procesadas 30000 filas...\n",
      "Procesadas 31000 filas...\n",
      "Procesadas 32000 filas...\n",
      "Procesadas 33000 filas...\n",
      "Procesadas 34000 filas...\n",
      "Procesadas 35000 filas...\n",
      "Procesadas 36000 filas...\n",
      "Procesadas 37000 filas...\n",
      "Procesadas 38000 filas...\n",
      "Procesadas 39000 filas...\n",
      "Procesadas 40000 filas...\n",
      "Procesadas 41000 filas...\n",
      "Procesadas 42000 filas...\n",
      "Procesadas 43000 filas...\n",
      "Procesadas 44000 filas...\n",
      "Procesadas 45000 filas...\n",
      "Procesadas 46000 filas...\n",
      "Procesadas 47000 filas...\n",
      "Procesadas 48000 filas...\n",
      "Procesadas 49000 filas...\n",
      "Procesadas 50000 filas...\n",
      "Procesadas 51000 filas...\n",
      "Procesadas 52000 filas...\n",
      "Procesadas 53000 filas...\n",
      "Procesadas 54000 filas...\n",
      "Procesadas 55000 filas...\n",
      "Procesadas 56000 filas...\n",
      "Procesadas 57000 filas...\n",
      "Procesadas 58000 filas...\n",
      "Procesadas 59000 filas...\n",
      "Procesadas 60000 filas...\n",
      "Procesadas 61000 filas...\n",
      "Procesadas 62000 filas...\n",
      "Procesadas 63000 filas...\n",
      "Procesadas 64000 filas...\n",
      "Procesadas 65000 filas...\n",
      "Procesadas 66000 filas...\n",
      "Procesadas 67000 filas...\n",
      "Procesadas 68000 filas...\n",
      "Procesadas 69000 filas...\n",
      "Procesadas 70000 filas...\n",
      "Procesadas 71000 filas...\n",
      "Procesadas 72000 filas...\n",
      "Procesadas 73000 filas...\n",
      "Procesadas 74000 filas...\n",
      "Procesadas 75000 filas...\n",
      "Procesadas 76000 filas...\n",
      "Procesadas 77000 filas...\n",
      "Procesadas 78000 filas...\n",
      "Procesadas 79000 filas...\n",
      "Procesadas 80000 filas...\n",
      "Procesadas 81000 filas...\n",
      "Procesadas 82000 filas...\n",
      "Procesadas 83000 filas...\n",
      "Procesadas 84000 filas...\n",
      "Procesadas 85000 filas...\n",
      "Procesadas 86000 filas...\n",
      "Procesadas 87000 filas...\n",
      "Procesadas 88000 filas...\n",
      "Procesadas 89000 filas...\n",
      "Procesadas 90000 filas...\n",
      "Procesadas 91000 filas...\n",
      "Procesadas 92000 filas...\n",
      "Procesadas 93000 filas...\n",
      "Procesadas 94000 filas...\n",
      "Procesadas 95000 filas...\n",
      "Procesadas 96000 filas...\n",
      "Procesadas 97000 filas...\n",
      "Procesadas 97417 filas...\n",
      "Proceso finalizado: No hay más datos para descargar.\n",
      "Conexión cerrada.\n"
     ]
    }
   ],
   "source": [
    "# URL base de la API\n",
    "# URL sin paginación - la paginación se hará construyendo dinámicamente el query\n",
    "base_url_api = \"https://www.datos.gov.co/resource/62tk-nxj5.csv\"\n",
    "\n",
    "# Parámetros\n",
    "limit = 1000  # Reducido a 500 para queries más rápidas\n",
    "offset = 0    # desplazamiento inicial\n",
    "all_data = [] # lista para almacenar los bloques\n",
    "\n",
    "# Configuración de los parámetros de conexión\n",
    "server = 'localhost\\\\SQLEXPRESS'  # O el punto '.' que usaste en SSMS\n",
    "database = 'EM_BOG'\n",
    "\n",
    "# Cadena de conexión usando Autenticación de Windows (Trusted_Connection)\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO dbo.presion (\n",
    "        codigo_estacion, codigo_sensor, fecha_observacion, valor_observado,\n",
    "        nombre_estacion, departamento, municipio, zona_hidrografica,\n",
    "        latitud, longitud, descripcion_sensor, unidad_medida\n",
    "    ) \n",
    "    VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "\n",
    "# Función para reintentos\n",
    "import time\n",
    "def request_with_retries(url, max_retries=3, timeout=60):\n",
    "    \"\"\"Intenta hacer la solicitud hasta 3 veces con timeout de 60 segundos\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout)\n",
    "            return response\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout en intento {attempt + 1}/{max_retries}. Esperando 5 segundos...\")\n",
    "            time.sleep(5)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"Error de conexión en intento {attempt + 1}/{max_retries}: {e}\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    raise Exception(f\"No se pudo conectar después de {max_retries} intentos\")\n",
    "\n",
    "try:\n",
    "    # 1. Establecer conexión inicial\n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.fast_executemany = True\n",
    "        \n",
    "        print(\"Iniciando proceso de descarga e ingesta...\")\n",
    "\n",
    "        while True:\n",
    "            # 2. Construir el query SoQL con LIMIT y OFFSET incluidos\n",
    "            soql_query = f\"\"\"SELECT\n",
    "  `codigoestacion`,\n",
    "  `codigosensor`,\n",
    "  `fechaobservacion`,\n",
    "  `valorobservado`,\n",
    "  `nombreestacion`,\n",
    "  `departamento`,\n",
    "  `municipio`,\n",
    "  `zonahidrografica`,\n",
    "  `latitud`,\n",
    "  `longitud`,\n",
    "  `descripcionsensor`,\n",
    "  `unidadmedida`\n",
    "WHERE\n",
    "  `fechaobservacion`\n",
    "    BETWEEN \"2025-12-01T00:00:00\" :: floating_timestamp\n",
    "    AND \"2025-12-31T23:59:59\" :: floating_timestamp\n",
    "  AND (caseless_eq(`departamento`, \"CUNDINAMARCA\") OR caseless_eq(`departamento`, \"BOYACÁ\"))\n",
    "LIMIT {limit}\n",
    "OFFSET {offset}\"\"\"\n",
    "            \n",
    "            # URL-encode el query\n",
    "            import urllib.parse\n",
    "            encoded_query = urllib.parse.quote(soql_query)\n",
    "            url = f\"{base_url_api}?$query={encoded_query}\"\n",
    "            \n",
    "            response = request_with_retries(url, max_retries=3, timeout=60)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error en la API: {response.status_code}\")\n",
    "                print(f\"Respuesta: {response.text}\")\n",
    "                break\n",
    "\n",
    "            # 3. Leer bloque como DataFrame\n",
    "            df_chunk = pd.read_csv(StringIO(response.text), dtype=str)\n",
    "            \n",
    "            if df_chunk.empty:\n",
    "                print(\"Proceso finalizado: No hay más datos para descargar.\")\n",
    "                break\n",
    "            \n",
    "            # Convertir tipos de datos\n",
    "            df_chunk[\"valorobservado\"] = df_chunk[\"valorobservado\"].astype(float)\n",
    "            df_chunk[\"latitud\"] = df_chunk[\"latitud\"].astype(float)\n",
    "            df_chunk[\"longitud\"] = df_chunk[\"longitud\"].astype(float)  \n",
    "            df_chunk[\"fechaobservacion\"] = pd.to_datetime(df_chunk[\"fechaobservacion\"])            \n",
    "            \n",
    "            # 4. Cargar bloque actual a la base de datos\n",
    "            try:\n",
    "                records = df_chunk.values.tolist()\n",
    "                cursor.executemany(insert_query, records)\n",
    "                conn.commit()  # Commit por bloque para asegurar persistencia\n",
    "                \n",
    "                print(f\"Procesadas {offset + len(df_chunk)} filas...\")\n",
    "                \n",
    "            except Exception as e_db:\n",
    "                print(f\"Error insertando bloque en offset {offset}: {e_db}\")\n",
    "                conn.rollback()\n",
    "                break\n",
    "\n",
    "            # 5. Incrementar offset para el siguiente bloque\n",
    "            offset += limit\n",
    "\n",
    "except pyodbc.Error as e_conn:\n",
    "    print(f\"Error de conexión a la base de datos: {e_conn}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Conexión cerrada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef1d14",
   "metadata": {},
   "source": [
    "## Dirección del viento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f93d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de descarga e ingesta...\n",
      "Procesadas 1000 filas...\n",
      "Procesadas 2000 filas...\n",
      "Procesadas 3000 filas...\n",
      "Procesadas 4000 filas...\n",
      "Procesadas 5000 filas...\n",
      "Procesadas 6000 filas...\n",
      "Procesadas 7000 filas...\n",
      "Procesadas 8000 filas...\n",
      "Procesadas 9000 filas...\n",
      "Procesadas 10000 filas...\n",
      "Procesadas 11000 filas...\n",
      "Procesadas 12000 filas...\n",
      "Procesadas 13000 filas...\n",
      "Procesadas 14000 filas...\n",
      "Procesadas 15000 filas...\n",
      "Procesadas 16000 filas...\n",
      "Procesadas 17000 filas...\n",
      "Procesadas 18000 filas...\n",
      "Procesadas 19000 filas...\n",
      "Procesadas 20000 filas...\n",
      "Procesadas 21000 filas...\n",
      "Procesadas 22000 filas...\n",
      "Procesadas 23000 filas...\n",
      "Procesadas 24000 filas...\n",
      "Procesadas 25000 filas...\n",
      "Procesadas 26000 filas...\n",
      "Procesadas 27000 filas...\n",
      "Procesadas 28000 filas...\n",
      "Procesadas 29000 filas...\n",
      "Procesadas 30000 filas...\n",
      "Procesadas 31000 filas...\n",
      "Procesadas 32000 filas...\n",
      "Procesadas 33000 filas...\n",
      "Procesadas 34000 filas...\n",
      "Procesadas 35000 filas...\n",
      "Procesadas 36000 filas...\n",
      "Procesadas 37000 filas...\n",
      "Procesadas 38000 filas...\n",
      "Procesadas 39000 filas...\n",
      "Procesadas 40000 filas...\n",
      "Procesadas 41000 filas...\n",
      "Procesadas 42000 filas...\n",
      "Procesadas 43000 filas...\n",
      "Procesadas 44000 filas...\n",
      "Procesadas 45000 filas...\n",
      "Procesadas 46000 filas...\n",
      "Procesadas 47000 filas...\n",
      "Procesadas 48000 filas...\n",
      "Procesadas 49000 filas...\n",
      "Procesadas 50000 filas...\n",
      "Procesadas 51000 filas...\n",
      "Procesadas 52000 filas...\n",
      "Procesadas 53000 filas...\n",
      "Procesadas 54000 filas...\n",
      "Procesadas 55000 filas...\n",
      "Procesadas 56000 filas...\n",
      "Procesadas 57000 filas...\n",
      "Procesadas 58000 filas...\n",
      "Procesadas 59000 filas...\n",
      "Procesadas 60000 filas...\n",
      "Procesadas 61000 filas...\n",
      "Procesadas 62000 filas...\n",
      "Procesadas 63000 filas...\n",
      "Procesadas 64000 filas...\n",
      "Procesadas 65000 filas...\n",
      "Procesadas 66000 filas...\n",
      "Procesadas 67000 filas...\n",
      "Procesadas 68000 filas...\n",
      "Procesadas 69000 filas...\n",
      "Procesadas 70000 filas...\n",
      "Procesadas 71000 filas...\n",
      "Procesadas 72000 filas...\n",
      "Procesadas 73000 filas...\n",
      "Procesadas 74000 filas...\n",
      "Procesadas 75000 filas...\n",
      "Procesadas 76000 filas...\n",
      "Procesadas 77000 filas...\n",
      "Procesadas 78000 filas...\n",
      "Procesadas 79000 filas...\n",
      "Procesadas 80000 filas...\n",
      "Procesadas 81000 filas...\n",
      "Procesadas 82000 filas...\n",
      "Procesadas 83000 filas...\n",
      "Procesadas 84000 filas...\n",
      "Procesadas 85000 filas...\n",
      "Procesadas 86000 filas...\n",
      "Procesadas 87000 filas...\n",
      "Procesadas 88000 filas...\n",
      "Procesadas 89000 filas...\n",
      "Procesadas 90000 filas...\n",
      "Procesadas 91000 filas...\n",
      "Procesadas 92000 filas...\n",
      "Procesadas 93000 filas...\n",
      "Procesadas 94000 filas...\n",
      "Procesadas 95000 filas...\n",
      "Procesadas 96000 filas...\n",
      "Procesadas 97000 filas...\n",
      "Procesadas 98000 filas...\n",
      "Procesadas 99000 filas...\n",
      "Procesadas 100000 filas...\n",
      "Procesadas 101000 filas...\n",
      "Procesadas 102000 filas...\n",
      "Procesadas 103000 filas...\n",
      "Procesadas 104000 filas...\n",
      "Procesadas 105000 filas...\n",
      "Procesadas 106000 filas...\n",
      "Procesadas 107000 filas...\n",
      "Procesadas 108000 filas...\n",
      "Procesadas 109000 filas...\n",
      "Procesadas 110000 filas...\n",
      "Procesadas 111000 filas...\n",
      "Procesadas 112000 filas...\n",
      "Procesadas 113000 filas...\n",
      "Procesadas 114000 filas...\n",
      "Procesadas 115000 filas...\n",
      "Procesadas 116000 filas...\n",
      "Procesadas 117000 filas...\n",
      "Procesadas 118000 filas...\n",
      "Procesadas 119000 filas...\n",
      "Procesadas 120000 filas...\n",
      "Procesadas 121000 filas...\n",
      "Procesadas 122000 filas...\n",
      "Procesadas 123000 filas...\n",
      "Procesadas 124000 filas...\n",
      "Procesadas 125000 filas...\n",
      "Procesadas 126000 filas...\n",
      "Procesadas 127000 filas...\n",
      "Procesadas 128000 filas...\n",
      "Procesadas 129000 filas...\n",
      "Procesadas 130000 filas...\n",
      "Procesadas 131000 filas...\n",
      "Procesadas 132000 filas...\n",
      "Procesadas 133000 filas...\n",
      "Procesadas 134000 filas...\n",
      "Procesadas 135000 filas...\n",
      "Procesadas 136000 filas...\n",
      "Procesadas 137000 filas...\n",
      "Procesadas 138000 filas...\n",
      "Procesadas 139000 filas...\n",
      "Procesadas 140000 filas...\n",
      "Procesadas 141000 filas...\n",
      "Procesadas 142000 filas...\n",
      "Procesadas 143000 filas...\n",
      "Procesadas 144000 filas...\n",
      "Procesadas 145000 filas...\n",
      "Procesadas 146000 filas...\n",
      "Procesadas 147000 filas...\n",
      "Procesadas 148000 filas...\n",
      "Procesadas 149000 filas...\n",
      "Procesadas 150000 filas...\n",
      "Procesadas 151000 filas...\n",
      "Procesadas 152000 filas...\n",
      "Procesadas 153000 filas...\n",
      "Procesadas 154000 filas...\n",
      "Procesadas 155000 filas...\n",
      "Procesadas 156000 filas...\n",
      "Procesadas 157000 filas...\n",
      "Procesadas 158000 filas...\n",
      "Procesadas 159000 filas...\n",
      "Procesadas 160000 filas...\n",
      "Procesadas 161000 filas...\n",
      "Procesadas 162000 filas...\n",
      "Procesadas 163000 filas...\n",
      "Procesadas 164000 filas...\n",
      "Procesadas 165000 filas...\n",
      "Procesadas 166000 filas...\n",
      "Procesadas 167000 filas...\n",
      "Procesadas 168000 filas...\n",
      "Procesadas 169000 filas...\n",
      "Procesadas 170000 filas...\n",
      "Procesadas 171000 filas...\n",
      "Procesadas 172000 filas...\n",
      "Procesadas 173000 filas...\n",
      "Procesadas 174000 filas...\n",
      "Procesadas 175000 filas...\n",
      "Procesadas 176000 filas...\n",
      "Procesadas 177000 filas...\n",
      "Procesadas 178000 filas...\n",
      "Procesadas 179000 filas...\n",
      "Procesadas 180000 filas...\n",
      "Procesadas 181000 filas...\n",
      "Procesadas 182000 filas...\n",
      "Procesadas 183000 filas...\n",
      "Procesadas 184000 filas...\n",
      "Procesadas 185000 filas...\n",
      "Procesadas 186000 filas...\n",
      "Procesadas 187000 filas...\n",
      "Procesadas 188000 filas...\n",
      "Procesadas 189000 filas...\n",
      "Procesadas 190000 filas...\n",
      "Procesadas 191000 filas...\n",
      "Procesadas 192000 filas...\n",
      "Procesadas 193000 filas...\n",
      "Procesadas 194000 filas...\n",
      "Procesadas 195000 filas...\n",
      "Procesadas 196000 filas...\n",
      "Procesadas 197000 filas...\n",
      "Procesadas 198000 filas...\n",
      "Procesadas 199000 filas...\n",
      "Procesadas 200000 filas...\n",
      "Procesadas 201000 filas...\n",
      "Procesadas 202000 filas...\n",
      "Procesadas 203000 filas...\n",
      "Procesadas 204000 filas...\n",
      "Procesadas 205000 filas...\n",
      "Procesadas 206000 filas...\n",
      "Procesadas 207000 filas...\n",
      "Procesadas 208000 filas...\n",
      "Procesadas 209000 filas...\n",
      "Procesadas 210000 filas...\n",
      "Procesadas 211000 filas...\n",
      "Procesadas 212000 filas...\n",
      "Procesadas 213000 filas...\n",
      "Procesadas 214000 filas...\n",
      "Procesadas 215000 filas...\n",
      "Procesadas 216000 filas...\n",
      "Procesadas 217000 filas...\n",
      "Procesadas 218000 filas...\n",
      "Procesadas 219000 filas...\n",
      "Procesadas 220000 filas...\n",
      "Procesadas 221000 filas...\n",
      "Procesadas 222000 filas...\n",
      "Procesadas 223000 filas...\n",
      "Procesadas 224000 filas...\n",
      "Procesadas 225000 filas...\n",
      "Procesadas 226000 filas...\n",
      "Procesadas 227000 filas...\n",
      "Procesadas 228000 filas...\n",
      "Procesadas 229000 filas...\n",
      "Procesadas 230000 filas...\n",
      "Procesadas 231000 filas...\n",
      "Procesadas 232000 filas...\n",
      "Procesadas 233000 filas...\n",
      "Procesadas 234000 filas...\n",
      "Procesadas 235000 filas...\n",
      "Procesadas 236000 filas...\n",
      "Procesadas 237000 filas...\n",
      "Procesadas 238000 filas...\n",
      "Procesadas 239000 filas...\n",
      "Procesadas 240000 filas...\n",
      "Procesadas 241000 filas...\n",
      "Procesadas 242000 filas...\n",
      "Procesadas 243000 filas...\n",
      "Procesadas 244000 filas...\n",
      "Procesadas 245000 filas...\n",
      "Procesadas 246000 filas...\n",
      "Procesadas 247000 filas...\n",
      "Procesadas 248000 filas...\n",
      "Procesadas 249000 filas...\n",
      "Procesadas 250000 filas...\n",
      "Procesadas 251000 filas...\n",
      "Procesadas 252000 filas...\n",
      "Procesadas 253000 filas...\n",
      "Procesadas 254000 filas...\n",
      "Procesadas 255000 filas...\n",
      "Procesadas 256000 filas...\n",
      "Procesadas 257000 filas...\n",
      "Procesadas 258000 filas...\n",
      "Procesadas 259000 filas...\n",
      "Procesadas 260000 filas...\n",
      "Procesadas 261000 filas...\n",
      "Procesadas 262000 filas...\n",
      "Procesadas 263000 filas...\n",
      "Procesadas 264000 filas...\n",
      "Procesadas 265000 filas...\n",
      "Procesadas 266000 filas...\n",
      "Procesadas 267000 filas...\n",
      "Procesadas 268000 filas...\n",
      "Procesadas 269000 filas...\n",
      "Procesadas 270000 filas...\n",
      "Procesadas 271000 filas...\n",
      "Procesadas 272000 filas...\n",
      "Procesadas 273000 filas...\n",
      "Procesadas 274000 filas...\n",
      "Procesadas 275000 filas...\n",
      "Procesadas 276000 filas...\n",
      "Procesadas 277000 filas...\n",
      "Procesadas 278000 filas...\n",
      "Procesadas 279000 filas...\n",
      "Procesadas 280000 filas...\n",
      "Procesadas 281000 filas...\n",
      "Procesadas 282000 filas...\n",
      "Procesadas 283000 filas...\n",
      "Procesadas 284000 filas...\n",
      "Procesadas 285000 filas...\n",
      "Procesadas 286000 filas...\n",
      "Procesadas 287000 filas...\n",
      "Procesadas 288000 filas...\n",
      "Procesadas 289000 filas...\n",
      "Procesadas 290000 filas...\n",
      "Procesadas 291000 filas...\n",
      "Procesadas 292000 filas...\n",
      "Procesadas 293000 filas...\n",
      "Procesadas 294000 filas...\n",
      "Procesadas 295000 filas...\n",
      "Procesadas 296000 filas...\n",
      "Procesadas 297000 filas...\n",
      "Procesadas 298000 filas...\n",
      "Procesadas 299000 filas...\n",
      "Procesadas 300000 filas...\n",
      "Procesadas 301000 filas...\n",
      "Procesadas 302000 filas...\n",
      "Procesadas 302419 filas...\n",
      "Proceso finalizado: No hay más datos para descargar.\n",
      "Conexión cerrada.\n"
     ]
    }
   ],
   "source": [
    "# URL base de la API\n",
    "# URL sin paginación - la paginación se hará construyendo dinámicamente el query\n",
    "base_url_api = \"https://www.datos.gov.co/resource/kiw7-v9ta.csv\"\n",
    "\n",
    "# Parámetros\n",
    "limit = 1000  # número máximo permitido por la API\n",
    "offset = 0    # desplazamiento inicial\n",
    "all_data = [] # lista para almacenar los bloques\n",
    "\n",
    "# Configuración de los parámetros de conexión\n",
    "server = 'localhost\\\\SQLEXPRESS'  # O el punto '.' que usaste en SSMS\n",
    "database = 'EM_BOG'\n",
    "\n",
    "# Cadena de conexión usando Autenticación de Windows (Trusted_Connection)\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO dbo.dir_viento (\n",
    "        codigo_estacion, codigo_sensor, fecha_observacion, valor_observado,\n",
    "        nombre_estacion, departamento, municipio, zona_hidrografica,\n",
    "        latitud, longitud, descripcion_sensor, unidad_medida\n",
    "    ) \n",
    "    VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # 1. Establecer conexión inicial\n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.fast_executemany = True\n",
    "        \n",
    "        print(\"Iniciando proceso de descarga e ingesta...\")\n",
    "\n",
    "        while True:\n",
    "            # 2. Construir el query SoQL con LIMIT y OFFSET incluidos\n",
    "            soql_query = f\"\"\"SELECT\n",
    "  `codigoestacion`,\n",
    "  `codigosensor`,\n",
    "  `fechaobservacion`,\n",
    "  `valorobservado`,\n",
    "  `nombreestacion`,\n",
    "  `departamento`,\n",
    "  `municipio`,\n",
    "  `zonahidrografica`,\n",
    "  `latitud`,\n",
    "  `longitud`,\n",
    "  `descripcionsensor`,\n",
    "  `unidadmedida`\n",
    "WHERE\n",
    "  `fechaobservacion`\n",
    "    BETWEEN \"2025-12-01T00:00:00\" :: floating_timestamp\n",
    "    AND \"2025-12-31T23:59:59\" :: floating_timestamp\n",
    "  AND (caseless_eq(`departamento`, \"CUNDINAMARCA\") OR caseless_eq(`departamento`, \"BOYACÁ\"))\n",
    "LIMIT {limit}\n",
    "OFFSET {offset}\"\"\"\n",
    "            \n",
    "            # URL-encode el query\n",
    "            import urllib.parse\n",
    "            encoded_query = urllib.parse.quote(soql_query)\n",
    "            url = f\"{base_url_api}?$query={encoded_query}\"\n",
    "            \n",
    "            response = requests.get(url, timeout=15)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error en la API: {response.status_code}\")\n",
    "                print(f\"Respuesta: {response.text}\")\n",
    "                break\n",
    "\n",
    "            # 3. Leer bloque como DataFrame\n",
    "            df_chunk = pd.read_csv(StringIO(response.text), dtype=str)\n",
    "            \n",
    "            if df_chunk.empty:\n",
    "                print(\"Proceso finalizado: No hay más datos para descargar.\")\n",
    "                break\n",
    "            \n",
    "            # Convertir tipos de datos\n",
    "            df_chunk[\"valorobservado\"] = df_chunk[\"valorobservado\"].astype(float)\n",
    "            df_chunk[\"latitud\"] = df_chunk[\"latitud\"].astype(float)\n",
    "            df_chunk[\"longitud\"] = df_chunk[\"longitud\"].astype(float)  \n",
    "            df_chunk[\"fechaobservacion\"] = pd.to_datetime(df_chunk[\"fechaobservacion\"])            \n",
    "            \n",
    "            # 4. Cargar bloque actual a la base de datos\n",
    "            try:\n",
    "                records = df_chunk.values.tolist()\n",
    "                cursor.executemany(insert_query, records)\n",
    "                conn.commit()  # Commit por bloque para asegurar persistencia\n",
    "                \n",
    "                print(f\"Procesadas {offset + len(df_chunk)} filas...\")\n",
    "                \n",
    "            except Exception as e_db:\n",
    "                print(f\"Error insertando bloque en offset {offset}: {e_db}\")\n",
    "                conn.rollback()\n",
    "                break\n",
    "\n",
    "            # 5. Incrementar offset para el siguiente bloque\n",
    "            offset += limit\n",
    "\n",
    "except pyodbc.Error as e_conn:\n",
    "    print(f\"Error de conexión a la base de datos: {e_conn}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Conexión cerrada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181a94bb",
   "metadata": {},
   "source": [
    "## Velocidad del viento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55103b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de descarga e ingesta...\n",
      "Procesadas 1000 filas...\n",
      "Procesadas 2000 filas...\n",
      "Procesadas 3000 filas...\n",
      "Procesadas 4000 filas...\n",
      "Procesadas 5000 filas...\n",
      "Procesadas 6000 filas...\n",
      "Procesadas 7000 filas...\n",
      "Procesadas 8000 filas...\n",
      "Procesadas 9000 filas...\n",
      "Procesadas 10000 filas...\n",
      "Procesadas 11000 filas...\n",
      "Procesadas 12000 filas...\n",
      "Procesadas 13000 filas...\n",
      "Procesadas 14000 filas...\n",
      "Procesadas 15000 filas...\n",
      "Procesadas 16000 filas...\n",
      "Procesadas 17000 filas...\n",
      "Procesadas 18000 filas...\n",
      "Procesadas 19000 filas...\n",
      "Procesadas 20000 filas...\n",
      "Procesadas 21000 filas...\n",
      "Procesadas 22000 filas...\n",
      "Procesadas 23000 filas...\n",
      "Procesadas 24000 filas...\n",
      "Procesadas 25000 filas...\n",
      "Procesadas 26000 filas...\n",
      "Procesadas 27000 filas...\n",
      "Procesadas 28000 filas...\n",
      "Procesadas 29000 filas...\n",
      "Procesadas 30000 filas...\n",
      "Procesadas 31000 filas...\n",
      "Procesadas 32000 filas...\n",
      "Procesadas 33000 filas...\n",
      "Procesadas 34000 filas...\n",
      "Procesadas 35000 filas...\n",
      "Procesadas 36000 filas...\n",
      "Procesadas 37000 filas...\n",
      "Procesadas 38000 filas...\n",
      "Procesadas 39000 filas...\n",
      "Procesadas 40000 filas...\n",
      "Procesadas 41000 filas...\n",
      "Procesadas 42000 filas...\n",
      "Procesadas 43000 filas...\n",
      "Procesadas 44000 filas...\n",
      "Procesadas 45000 filas...\n",
      "Procesadas 46000 filas...\n",
      "Procesadas 47000 filas...\n",
      "Procesadas 48000 filas...\n",
      "Procesadas 49000 filas...\n",
      "Procesadas 50000 filas...\n",
      "Procesadas 51000 filas...\n",
      "Procesadas 52000 filas...\n",
      "Procesadas 53000 filas...\n",
      "Procesadas 54000 filas...\n",
      "Procesadas 55000 filas...\n",
      "Procesadas 56000 filas...\n",
      "Procesadas 57000 filas...\n",
      "Procesadas 58000 filas...\n",
      "Procesadas 59000 filas...\n",
      "Procesadas 60000 filas...\n",
      "Procesadas 61000 filas...\n",
      "Procesadas 62000 filas...\n",
      "Procesadas 63000 filas...\n",
      "Procesadas 64000 filas...\n",
      "Procesadas 65000 filas...\n",
      "Procesadas 66000 filas...\n",
      "Procesadas 67000 filas...\n",
      "Procesadas 68000 filas...\n",
      "Procesadas 69000 filas...\n",
      "Procesadas 70000 filas...\n",
      "Procesadas 71000 filas...\n",
      "Procesadas 72000 filas...\n",
      "Procesadas 73000 filas...\n",
      "Procesadas 74000 filas...\n",
      "Procesadas 75000 filas...\n",
      "Procesadas 76000 filas...\n",
      "Procesadas 77000 filas...\n",
      "Procesadas 78000 filas...\n",
      "Procesadas 79000 filas...\n",
      "Procesadas 80000 filas...\n",
      "Procesadas 81000 filas...\n",
      "Procesadas 82000 filas...\n",
      "Procesadas 83000 filas...\n",
      "Procesadas 84000 filas...\n",
      "Procesadas 85000 filas...\n",
      "Procesadas 86000 filas...\n",
      "Procesadas 87000 filas...\n",
      "Procesadas 88000 filas...\n",
      "Procesadas 89000 filas...\n",
      "Procesadas 90000 filas...\n",
      "Procesadas 91000 filas...\n",
      "Procesadas 92000 filas...\n",
      "Procesadas 93000 filas...\n",
      "Procesadas 94000 filas...\n",
      "Procesadas 95000 filas...\n",
      "Procesadas 96000 filas...\n",
      "Procesadas 97000 filas...\n",
      "Procesadas 98000 filas...\n",
      "Procesadas 99000 filas...\n",
      "Procesadas 100000 filas...\n",
      "Procesadas 101000 filas...\n",
      "Procesadas 102000 filas...\n",
      "Procesadas 103000 filas...\n",
      "Procesadas 104000 filas...\n",
      "Procesadas 105000 filas...\n",
      "Procesadas 106000 filas...\n",
      "Procesadas 107000 filas...\n",
      "Procesadas 108000 filas...\n",
      "Procesadas 109000 filas...\n",
      "Procesadas 110000 filas...\n",
      "Procesadas 111000 filas...\n",
      "Procesadas 112000 filas...\n",
      "Procesadas 113000 filas...\n",
      "Procesadas 114000 filas...\n",
      "Procesadas 115000 filas...\n",
      "Procesadas 116000 filas...\n",
      "Procesadas 117000 filas...\n",
      "Procesadas 118000 filas...\n",
      "Procesadas 119000 filas...\n",
      "Procesadas 120000 filas...\n",
      "Procesadas 121000 filas...\n",
      "Procesadas 122000 filas...\n",
      "Procesadas 123000 filas...\n",
      "Procesadas 124000 filas...\n",
      "Procesadas 125000 filas...\n",
      "Procesadas 126000 filas...\n",
      "Procesadas 127000 filas...\n",
      "Procesadas 128000 filas...\n",
      "Procesadas 129000 filas...\n",
      "Procesadas 130000 filas...\n",
      "Procesadas 131000 filas...\n",
      "Procesadas 132000 filas...\n",
      "Procesadas 133000 filas...\n",
      "Procesadas 134000 filas...\n",
      "Procesadas 135000 filas...\n",
      "Procesadas 136000 filas...\n",
      "Procesadas 137000 filas...\n",
      "Procesadas 138000 filas...\n",
      "Procesadas 139000 filas...\n",
      "Procesadas 140000 filas...\n",
      "Procesadas 141000 filas...\n",
      "Procesadas 142000 filas...\n",
      "Procesadas 143000 filas...\n",
      "Procesadas 144000 filas...\n",
      "Procesadas 145000 filas...\n",
      "Procesadas 146000 filas...\n",
      "Procesadas 147000 filas...\n",
      "Procesadas 148000 filas...\n",
      "Procesadas 149000 filas...\n",
      "Procesadas 150000 filas...\n",
      "Procesadas 151000 filas...\n",
      "Procesadas 152000 filas...\n",
      "Procesadas 153000 filas...\n",
      "Procesadas 154000 filas...\n",
      "Procesadas 155000 filas...\n",
      "Procesadas 156000 filas...\n",
      "Procesadas 157000 filas...\n",
      "Procesadas 158000 filas...\n",
      "Procesadas 159000 filas...\n",
      "Procesadas 160000 filas...\n",
      "Procesadas 161000 filas...\n",
      "Procesadas 162000 filas...\n",
      "Procesadas 163000 filas...\n",
      "Procesadas 164000 filas...\n",
      "Procesadas 165000 filas...\n",
      "Procesadas 166000 filas...\n",
      "Procesadas 167000 filas...\n",
      "Procesadas 168000 filas...\n",
      "Procesadas 169000 filas...\n",
      "Procesadas 170000 filas...\n",
      "Procesadas 171000 filas...\n",
      "Procesadas 172000 filas...\n",
      "Procesadas 173000 filas...\n",
      "Procesadas 174000 filas...\n",
      "Procesadas 175000 filas...\n",
      "Procesadas 176000 filas...\n",
      "Procesadas 177000 filas...\n",
      "Procesadas 178000 filas...\n",
      "Procesadas 179000 filas...\n",
      "Procesadas 180000 filas...\n",
      "Procesadas 181000 filas...\n",
      "Procesadas 182000 filas...\n",
      "Procesadas 183000 filas...\n",
      "Procesadas 184000 filas...\n",
      "Procesadas 185000 filas...\n",
      "Procesadas 186000 filas...\n",
      "Procesadas 187000 filas...\n",
      "Procesadas 188000 filas...\n",
      "Procesadas 189000 filas...\n",
      "Procesadas 190000 filas...\n",
      "Procesadas 191000 filas...\n",
      "Procesadas 192000 filas...\n",
      "Procesadas 193000 filas...\n",
      "Procesadas 194000 filas...\n",
      "Procesadas 195000 filas...\n",
      "Procesadas 196000 filas...\n",
      "Procesadas 197000 filas...\n",
      "Procesadas 198000 filas...\n",
      "Procesadas 199000 filas...\n",
      "Procesadas 200000 filas...\n",
      "Procesadas 201000 filas...\n",
      "Procesadas 202000 filas...\n",
      "Procesadas 203000 filas...\n",
      "Procesadas 204000 filas...\n",
      "Procesadas 205000 filas...\n",
      "Procesadas 206000 filas...\n",
      "Procesadas 207000 filas...\n",
      "Procesadas 208000 filas...\n",
      "Procesadas 209000 filas...\n",
      "Procesadas 210000 filas...\n",
      "Procesadas 211000 filas...\n",
      "Procesadas 212000 filas...\n",
      "Procesadas 213000 filas...\n",
      "Procesadas 214000 filas...\n",
      "Procesadas 215000 filas...\n",
      "Procesadas 216000 filas...\n",
      "Procesadas 217000 filas...\n",
      "Procesadas 218000 filas...\n",
      "Procesadas 219000 filas...\n",
      "Procesadas 220000 filas...\n",
      "Procesadas 221000 filas...\n",
      "Procesadas 222000 filas...\n",
      "Procesadas 223000 filas...\n",
      "Procesadas 224000 filas...\n",
      "Procesadas 225000 filas...\n",
      "Procesadas 226000 filas...\n",
      "Procesadas 227000 filas...\n",
      "Procesadas 228000 filas...\n",
      "Procesadas 229000 filas...\n",
      "Procesadas 230000 filas...\n",
      "Procesadas 231000 filas...\n",
      "Procesadas 232000 filas...\n",
      "Procesadas 233000 filas...\n",
      "Procesadas 234000 filas...\n",
      "Procesadas 235000 filas...\n",
      "Procesadas 236000 filas...\n",
      "Procesadas 237000 filas...\n",
      "Procesadas 238000 filas...\n",
      "Procesadas 239000 filas...\n",
      "Procesadas 240000 filas...\n",
      "Procesadas 241000 filas...\n",
      "Procesadas 242000 filas...\n",
      "Procesadas 243000 filas...\n",
      "Procesadas 244000 filas...\n",
      "Procesadas 245000 filas...\n",
      "Procesadas 246000 filas...\n",
      "Procesadas 247000 filas...\n",
      "Procesadas 248000 filas...\n",
      "Procesadas 249000 filas...\n",
      "Procesadas 250000 filas...\n",
      "Procesadas 251000 filas...\n",
      "Procesadas 252000 filas...\n",
      "Procesadas 253000 filas...\n",
      "Procesadas 254000 filas...\n",
      "Procesadas 255000 filas...\n",
      "Procesadas 256000 filas...\n",
      "Procesadas 257000 filas...\n",
      "Procesadas 258000 filas...\n",
      "Procesadas 259000 filas...\n",
      "Procesadas 260000 filas...\n",
      "Procesadas 261000 filas...\n",
      "Procesadas 262000 filas...\n",
      "Procesadas 263000 filas...\n",
      "Procesadas 264000 filas...\n",
      "Procesadas 265000 filas...\n",
      "Procesadas 266000 filas...\n",
      "Procesadas 267000 filas...\n",
      "Procesadas 268000 filas...\n",
      "Procesadas 269000 filas...\n",
      "Procesadas 270000 filas...\n",
      "Procesadas 271000 filas...\n",
      "Procesadas 272000 filas...\n",
      "Procesadas 273000 filas...\n",
      "Procesadas 274000 filas...\n",
      "Procesadas 275000 filas...\n",
      "Procesadas 276000 filas...\n",
      "Procesadas 277000 filas...\n",
      "Procesadas 278000 filas...\n",
      "Procesadas 279000 filas...\n",
      "Procesadas 280000 filas...\n",
      "Procesadas 281000 filas...\n",
      "Procesadas 282000 filas...\n",
      "Procesadas 283000 filas...\n",
      "Procesadas 284000 filas...\n",
      "Procesadas 285000 filas...\n",
      "Procesadas 286000 filas...\n",
      "Procesadas 287000 filas...\n",
      "Procesadas 288000 filas...\n",
      "Procesadas 289000 filas...\n",
      "Procesadas 290000 filas...\n",
      "Procesadas 291000 filas...\n",
      "Procesadas 292000 filas...\n",
      "Procesadas 293000 filas...\n",
      "Procesadas 294000 filas...\n",
      "Procesadas 295000 filas...\n",
      "Procesadas 296000 filas...\n",
      "Procesadas 297000 filas...\n",
      "Procesadas 298000 filas...\n",
      "Procesadas 299000 filas...\n",
      "Procesadas 300000 filas...\n",
      "Procesadas 301000 filas...\n",
      "Procesadas 302000 filas...\n",
      "Procesadas 303000 filas...\n",
      "Procesadas 304000 filas...\n",
      "Procesadas 305000 filas...\n",
      "Procesadas 306000 filas...\n",
      "Procesadas 307000 filas...\n",
      "Procesadas 308000 filas...\n",
      "Procesadas 309000 filas...\n",
      "Procesadas 310000 filas...\n",
      "Procesadas 311000 filas...\n",
      "Procesadas 312000 filas...\n",
      "Procesadas 313000 filas...\n",
      "Procesadas 314000 filas...\n",
      "Procesadas 315000 filas...\n",
      "Procesadas 316000 filas...\n",
      "Procesadas 317000 filas...\n",
      "Procesadas 318000 filas...\n",
      "Procesadas 319000 filas...\n",
      "Procesadas 320000 filas...\n",
      "Procesadas 321000 filas...\n",
      "Procesadas 322000 filas...\n",
      "Procesadas 323000 filas...\n",
      "Procesadas 324000 filas...\n",
      "Procesadas 324690 filas...\n",
      "Proceso finalizado: No hay más datos para descargar.\n",
      "Conexión cerrada.\n"
     ]
    }
   ],
   "source": [
    "# URL base de la API\n",
    "# URL sin paginación - la paginación se hará construyendo dinámicamente el query\n",
    "base_url_api = \"https://www.datos.gov.co/resource/sgfv-3yp8.csv\"\n",
    "\n",
    "# Parámetros\n",
    "limit = 1000  # número máximo permitido por la API\n",
    "offset = 0    # desplazamiento inicial\n",
    "all_data = [] # lista para almacenar los bloques\n",
    "\n",
    "# Configuración de los parámetros de conexión\n",
    "server = 'localhost\\\\SQLEXPRESS'  # O el punto '.' que usaste en SSMS\n",
    "database = 'EM_BOG'\n",
    "\n",
    "# Cadena de conexión usando Autenticación de Windows (Trusted_Connection)\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO dbo.vel_viento (\n",
    "        codigo_estacion, codigo_sensor, fecha_observacion, valor_observado,\n",
    "        nombre_estacion, departamento, municipio, zona_hidrografica,\n",
    "        latitud, longitud, descripcion_sensor, unidad_medida\n",
    "    ) \n",
    "    VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # 1. Establecer conexión inicial\n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.fast_executemany = True\n",
    "        \n",
    "        print(\"Iniciando proceso de descarga e ingesta...\")\n",
    "\n",
    "        while True:\n",
    "            # 2. Construir el query SoQL con LIMIT y OFFSET incluidos\n",
    "            soql_query = f\"\"\"SELECT\n",
    "  `codigoestacion`,\n",
    "  `codigosensor`,\n",
    "  `fechaobservacion`,\n",
    "  `valorobservado`,\n",
    "  `nombreestacion`,\n",
    "  `departamento`,\n",
    "  `municipio`,\n",
    "  `zonahidrografica`,\n",
    "  `latitud`,\n",
    "  `longitud`,\n",
    "  `descripcionsensor`,\n",
    "  `unidadmedida`\n",
    "WHERE\n",
    "  `fechaobservacion`\n",
    "    BETWEEN \"2025-12-01T00:00:00\" :: floating_timestamp\n",
    "    AND \"2025-12-31T23:59:59\" :: floating_timestamp\n",
    "  AND (caseless_eq(`departamento`, \"CUNDINAMARCA\") OR caseless_eq(`departamento`, \"BOYACÁ\"))\n",
    "LIMIT {limit}\n",
    "OFFSET {offset}\"\"\"\n",
    "            \n",
    "            # URL-encode el query\n",
    "            import urllib.parse\n",
    "            encoded_query = urllib.parse.quote(soql_query)\n",
    "            url = f\"{base_url_api}?$query={encoded_query}\"\n",
    "            \n",
    "            response = requests.get(url, timeout=15)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error en la API: {response.status_code}\")\n",
    "                print(f\"Respuesta: {response.text}\")\n",
    "                break\n",
    "\n",
    "            # 3. Leer bloque como DataFrame\n",
    "            df_chunk = pd.read_csv(StringIO(response.text), dtype=str)\n",
    "            \n",
    "            if df_chunk.empty:\n",
    "                print(\"Proceso finalizado: No hay más datos para descargar.\")\n",
    "                break\n",
    "            \n",
    "            # Convertir tipos de datos\n",
    "            df_chunk[\"valorobservado\"] = df_chunk[\"valorobservado\"].astype(float)\n",
    "            df_chunk[\"latitud\"] = df_chunk[\"latitud\"].astype(float)\n",
    "            df_chunk[\"longitud\"] = df_chunk[\"longitud\"].astype(float)  \n",
    "            df_chunk[\"fechaobservacion\"] = pd.to_datetime(df_chunk[\"fechaobservacion\"])            \n",
    "            \n",
    "            # 4. Cargar bloque actual a la base de datos\n",
    "            try:\n",
    "                records = df_chunk.values.tolist()\n",
    "                cursor.executemany(insert_query, records)\n",
    "                conn.commit()  # Commit por bloque para asegurar persistencia\n",
    "                \n",
    "                print(f\"Procesadas {offset + len(df_chunk)} filas...\")\n",
    "                \n",
    "            except Exception as e_db:\n",
    "                print(f\"Error insertando bloque en offset {offset}: {e_db}\")\n",
    "                conn.rollback()\n",
    "                break\n",
    "\n",
    "            # 5. Incrementar offset para el siguiente bloque\n",
    "            offset += limit\n",
    "\n",
    "except pyodbc.Error as e_conn:\n",
    "    print(f\"Error de conexión a la base de datos: {e_conn}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Conexión cerrada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fdbe50",
   "metadata": {},
   "source": [
    "## Temperatura del aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce53e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de descarga e ingesta...\n",
      "Procesadas 1000 filas...\n",
      "Procesadas 2000 filas...\n",
      "Procesadas 3000 filas...\n",
      "Procesadas 4000 filas...\n",
      "Procesadas 5000 filas...\n",
      "Procesadas 6000 filas...\n",
      "Procesadas 7000 filas...\n",
      "Procesadas 8000 filas...\n",
      "Procesadas 9000 filas...\n",
      "Procesadas 10000 filas...\n",
      "Procesadas 11000 filas...\n",
      "Procesadas 12000 filas...\n",
      "Procesadas 13000 filas...\n",
      "Procesadas 14000 filas...\n",
      "Procesadas 15000 filas...\n",
      "Procesadas 16000 filas...\n",
      "Procesadas 17000 filas...\n",
      "Procesadas 18000 filas...\n",
      "Procesadas 19000 filas...\n",
      "Procesadas 20000 filas...\n",
      "Procesadas 21000 filas...\n",
      "Procesadas 22000 filas...\n",
      "Procesadas 23000 filas...\n",
      "Procesadas 24000 filas...\n",
      "Procesadas 25000 filas...\n",
      "Procesadas 26000 filas...\n",
      "Procesadas 27000 filas...\n",
      "Procesadas 28000 filas...\n",
      "Procesadas 29000 filas...\n",
      "Procesadas 30000 filas...\n",
      "Procesadas 31000 filas...\n",
      "Procesadas 32000 filas...\n",
      "Procesadas 33000 filas...\n",
      "Procesadas 34000 filas...\n",
      "Procesadas 35000 filas...\n",
      "Procesadas 36000 filas...\n",
      "Procesadas 37000 filas...\n",
      "Procesadas 38000 filas...\n",
      "Procesadas 39000 filas...\n",
      "Procesadas 40000 filas...\n",
      "Procesadas 41000 filas...\n",
      "Procesadas 42000 filas...\n",
      "Procesadas 43000 filas...\n",
      "Procesadas 44000 filas...\n",
      "Procesadas 45000 filas...\n",
      "Procesadas 46000 filas...\n",
      "Procesadas 47000 filas...\n",
      "Procesadas 48000 filas...\n",
      "Procesadas 49000 filas...\n",
      "Procesadas 50000 filas...\n",
      "Procesadas 51000 filas...\n",
      "Procesadas 52000 filas...\n",
      "Procesadas 53000 filas...\n",
      "Procesadas 54000 filas...\n",
      "Procesadas 55000 filas...\n",
      "Procesadas 56000 filas...\n",
      "Procesadas 57000 filas...\n",
      "Procesadas 58000 filas...\n",
      "Procesadas 59000 filas...\n",
      "Procesadas 60000 filas...\n",
      "Procesadas 61000 filas...\n",
      "Procesadas 62000 filas...\n",
      "Procesadas 63000 filas...\n",
      "Procesadas 64000 filas...\n",
      "Procesadas 65000 filas...\n",
      "Procesadas 66000 filas...\n",
      "Procesadas 67000 filas...\n",
      "Procesadas 68000 filas...\n",
      "Procesadas 69000 filas...\n",
      "Procesadas 70000 filas...\n",
      "Procesadas 71000 filas...\n",
      "Procesadas 72000 filas...\n",
      "Procesadas 73000 filas...\n",
      "Procesadas 74000 filas...\n",
      "Procesadas 75000 filas...\n",
      "Procesadas 76000 filas...\n",
      "Procesadas 77000 filas...\n",
      "Procesadas 78000 filas...\n",
      "Procesadas 79000 filas...\n",
      "Procesadas 80000 filas...\n",
      "Procesadas 81000 filas...\n",
      "Procesadas 82000 filas...\n",
      "Procesadas 83000 filas...\n",
      "Procesadas 84000 filas...\n",
      "Procesadas 85000 filas...\n",
      "Procesadas 86000 filas...\n",
      "Procesadas 87000 filas...\n",
      "Procesadas 88000 filas...\n",
      "Procesadas 89000 filas...\n",
      "Procesadas 90000 filas...\n",
      "Procesadas 91000 filas...\n",
      "Procesadas 92000 filas...\n",
      "Procesadas 93000 filas...\n",
      "Procesadas 94000 filas...\n",
      "Procesadas 95000 filas...\n",
      "Procesadas 96000 filas...\n",
      "Procesadas 97000 filas...\n",
      "Procesadas 98000 filas...\n",
      "Procesadas 99000 filas...\n",
      "Procesadas 100000 filas...\n",
      "Procesadas 101000 filas...\n",
      "Procesadas 102000 filas...\n",
      "Procesadas 103000 filas...\n",
      "Procesadas 104000 filas...\n",
      "Procesadas 105000 filas...\n",
      "Procesadas 106000 filas...\n",
      "Procesadas 107000 filas...\n",
      "Procesadas 108000 filas...\n",
      "Procesadas 109000 filas...\n",
      "Procesadas 110000 filas...\n",
      "Procesadas 111000 filas...\n",
      "Procesadas 112000 filas...\n",
      "Procesadas 113000 filas...\n",
      "Procesadas 114000 filas...\n",
      "Procesadas 115000 filas...\n",
      "Procesadas 116000 filas...\n",
      "Procesadas 117000 filas...\n",
      "Procesadas 118000 filas...\n",
      "Procesadas 119000 filas...\n",
      "Procesadas 120000 filas...\n",
      "Procesadas 121000 filas...\n",
      "Procesadas 122000 filas...\n",
      "Procesadas 123000 filas...\n",
      "Procesadas 124000 filas...\n",
      "Procesadas 125000 filas...\n",
      "Procesadas 125695 filas...\n",
      "Proceso finalizado: No hay más datos para descargar.\n",
      "Conexión cerrada.\n"
     ]
    }
   ],
   "source": [
    "# URL base de la API\n",
    "# URL sin paginación - la paginación se hará construyendo dinámicamente el query\n",
    "base_url_api = \"https://www.datos.gov.co/resource/sbwg-7ju4.csv\"\n",
    "\n",
    "# Parámetros\n",
    "limit = 1000  # número máximo permitido por la API\n",
    "offset = 0    # desplazamiento inicial\n",
    "all_data = [] # lista para almacenar los bloques\n",
    "\n",
    "# Configuración de los parámetros de conexión\n",
    "server = 'localhost\\\\SQLEXPRESS'  # O el punto '.' que usaste en SSMS\n",
    "database = 'EM_BOG'\n",
    "\n",
    "# Cadena de conexión usando Autenticación de Windows (Trusted_Connection)\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO dbo.temp_aire (\n",
    "        codigo_estacion, codigo_sensor, fecha_observacion, valor_observado,\n",
    "        nombre_estacion, departamento, municipio, zona_hidrografica,\n",
    "        latitud, longitud, descripcion_sensor, unidad_medida\n",
    "    ) \n",
    "    VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # 1. Establecer conexión inicial\n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.fast_executemany = True\n",
    "        \n",
    "        print(\"Iniciando proceso de descarga e ingesta...\")\n",
    "\n",
    "        while True:\n",
    "            # 2. Construir el query SoQL con LIMIT y OFFSET incluidos\n",
    "            soql_query = f\"\"\"SELECT\n",
    "  `codigoestacion`,\n",
    "  `codigosensor`,\n",
    "  `fechaobservacion`,\n",
    "  `valorobservado`,\n",
    "  `nombreestacion`,\n",
    "  `departamento`,\n",
    "  `municipio`,\n",
    "  `zonahidrografica`,\n",
    "  `latitud`,\n",
    "  `longitud`,\n",
    "  `descripcionsensor`,\n",
    "  `unidadmedida`\n",
    "WHERE\n",
    "  `fechaobservacion`\n",
    "    BETWEEN \"2025-12-01T00:00:00\" :: floating_timestamp\n",
    "    AND \"2025-12-31T23:59:59\" :: floating_timestamp\n",
    "  AND (caseless_eq(`departamento`, \"CUNDINAMARCA\") OR caseless_eq(`departamento`, \"BOYACÁ\"))\n",
    "LIMIT {limit}\n",
    "OFFSET {offset}\"\"\"\n",
    "            \n",
    "            # URL-encode el query\n",
    "            import urllib.parse\n",
    "            encoded_query = urllib.parse.quote(soql_query)\n",
    "            url = f\"{base_url_api}?$query={encoded_query}\"\n",
    "            \n",
    "            response = requests.get(url, timeout=15)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error en la API: {response.status_code}\")\n",
    "                print(f\"Respuesta: {response.text}\")\n",
    "                break\n",
    "\n",
    "            # 3. Leer bloque como DataFrame\n",
    "            df_chunk = pd.read_csv(StringIO(response.text), dtype=str)\n",
    "            \n",
    "            if df_chunk.empty:\n",
    "                print(\"Proceso finalizado: No hay más datos para descargar.\")\n",
    "                break\n",
    "            \n",
    "            # Convertir tipos de datos\n",
    "            df_chunk[\"valorobservado\"] = df_chunk[\"valorobservado\"].astype(float)\n",
    "            df_chunk[\"latitud\"] = df_chunk[\"latitud\"].astype(float)\n",
    "            df_chunk[\"longitud\"] = df_chunk[\"longitud\"].astype(float)  \n",
    "            df_chunk[\"fechaobservacion\"] = pd.to_datetime(df_chunk[\"fechaobservacion\"])            \n",
    "            \n",
    "            # 4. Cargar bloque actual a la base de datos\n",
    "            try:\n",
    "                records = df_chunk.values.tolist()\n",
    "                cursor.executemany(insert_query, records)\n",
    "                conn.commit()  # Commit por bloque para asegurar persistencia\n",
    "                \n",
    "                print(f\"Procesadas {offset + len(df_chunk)} filas...\")\n",
    "                \n",
    "            except Exception as e_db:\n",
    "                print(f\"Error insertando bloque en offset {offset}: {e_db}\")\n",
    "                conn.rollback()\n",
    "                break\n",
    "\n",
    "            # 5. Incrementar offset para el siguiente bloque\n",
    "            offset += limit\n",
    "\n",
    "except pyodbc.Error as e_conn:\n",
    "    print(f\"Error de conexión a la base de datos: {e_conn}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Conexión cerrada.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
